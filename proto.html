<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.3 prototype と criticism | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="6.3 prototype と criticism | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.3 prototype と criticism | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2021-05-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="adversarial.html"/>
<link rel="next" href="influential.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>要約</a></li>
<li class="chapter" data-level="" data-path="著者による序文.html"><a href="著者による序文.html"><i class="fa fa-check"></i>著者による序文</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> イントロダクション</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> 物語の時間</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#稲妻は二度と打たない"><i class="fa fa-check"></i>稲妻は二度と打たない</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#信用失墜"><i class="fa fa-check"></i>信用失墜</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#フェルミのペーパークリップ"><i class="fa fa-check"></i>フェルミのペーパー・クリップ</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="機械学習とは何か.html"><a href="機械学習とは何か.html"><i class="fa fa-check"></i><b>1.2</b> 機械学習とは何か？</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> 専門用語</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> 解釈可能性</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> 解釈可能性の重要性</a></li>
<li class="chapter" data-level="2.2" data-path="解釈可能な手法の分類.html"><a href="解釈可能な手法の分類.html"><i class="fa fa-check"></i><b>2.2</b> 解釈可能な手法の分類</a></li>
<li class="chapter" data-level="2.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html"><i class="fa fa-check"></i><b>2.3</b> 解釈可能性の範囲</a><ul>
<li class="chapter" data-level="2.3.1" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#アルゴリズムの透明性"><i class="fa fa-check"></i><b>2.3.1</b> アルゴリズムの透明性</a></li>
<li class="chapter" data-level="2.3.2" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#全体的なモデルの解釈可能性"><i class="fa fa-check"></i><b>2.3.2</b> 全体的なモデルの解釈可能性</a></li>
<li class="chapter" data-level="2.3.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#モジュールレベルのモデルの全体的な解釈可能性"><i class="fa fa-check"></i><b>2.3.3</b> モジュールレベルのモデルの全体的な解釈可能性</a></li>
<li class="chapter" data-level="2.3.4" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#単一の予測に対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.4</b> 単一の予測に対する局所的な解釈</a></li>
<li class="chapter" data-level="2.3.5" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#予測のグループに対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.5</b> 予測のグループに対する局所的な解釈</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="解釈可能性の評価.html"><a href="解釈可能性の評価.html"><i class="fa fa-check"></i><b>2.4</b> 解釈可能性の評価</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> 説明に関する性質</a></li>
<li class="chapter" data-level="2.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.6</b> 人間に優しい説明</a><ul>
<li class="chapter" data-level="2.6.1" data-path="explanation.html"><a href="explanation.html#説明とはなにか"><i class="fa fa-check"></i><b>2.6.1</b> 説明とはなにか</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> データセット</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> 自転車レンタル (回帰)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube スパムコメント (テキスト分類)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> 子宮頸がんのリスク要因(クラス分類)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> 解釈可能なモデル</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> 線形回帰</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#解釈"><i class="fa fa-check"></i><b>4.1.1</b> 解釈</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#例"><i class="fa fa-check"></i><b>4.1.2</b> 例</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#可視化による解釈"><i class="fa fa-check"></i><b>4.1.3</b> 可視化による解釈</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#個々の予測に対する説明"><i class="fa fa-check"></i><b>4.1.4</b> 個々の予測に対する説明</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#カテゴリカル特徴量のエンコーディング"><i class="fa fa-check"></i><b>4.1.5</b> カテゴリカル特徴量のエンコーディング</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#線形モデルは良い説明を与えるか"><i class="fa fa-check"></i><b>4.1.6</b> 線形モデルは良い説明を与えるか?</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.7</b> スパースな線形モデル</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#長所"><i class="fa fa-check"></i><b>4.1.8</b> 長所</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#短所"><i class="fa fa-check"></i><b>4.1.9</b> 短所</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> ロジスティック回帰</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#線形回帰モデルを分類のために使うと何がいけないか"><i class="fa fa-check"></i><b>4.2.1</b> 線形回帰モデルを分類のために使うと何がいけないか。</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic.html"><a href="logistic.html#理論"><i class="fa fa-check"></i><b>4.2.2</b> 理論</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic.html"><a href="logistic.html#解釈性"><i class="fa fa-check"></i><b>4.2.3</b> 解釈性</a></li>
<li class="chapter" data-level="4.2.4" data-path="logistic.html"><a href="logistic.html#例-1"><i class="fa fa-check"></i><b>4.2.4</b> 例</a></li>
<li class="chapter" data-level="4.2.5" data-path="logistic.html"><a href="logistic.html#長所と短所"><i class="fa fa-check"></i><b>4.2.5</b> 長所と短所</a></li>
<li class="chapter" data-level="4.2.6" data-path="logistic.html"><a href="logistic.html#ソフトウェア"><i class="fa fa-check"></i><b>4.2.6</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>4.3</b> GLM、GAM、その他</a><ul>
<li class="chapter" data-level="4.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>4.3.1</b> 結果が正規分布に従わない場合 - GLMs</a></li>
<li class="chapter" data-level="4.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>4.3.2</b> 相互作用</a></li>
<li class="chapter" data-level="4.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>4.3.3</b> 非線形効果 - GAM</a></li>
<li class="chapter" data-level="4.3.4" data-path="extend-lm.html"><a href="extend-lm.html#長所-1"><i class="fa fa-check"></i><b>4.3.4</b> 長所</a></li>
<li class="chapter" data-level="4.3.5" data-path="extend-lm.html"><a href="extend-lm.html#短所-1"><i class="fa fa-check"></i><b>4.3.5</b> 短所</a></li>
<li class="chapter" data-level="4.3.6" data-path="extend-lm.html"><a href="extend-lm.html#ソフトウェア-1"><i class="fa fa-check"></i><b>4.3.6</b> ソフトウェア</a></li>
<li class="chapter" data-level="4.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>4.3.7</b> さらなる拡張</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.4</b> 決定木</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree.html"><a href="tree.html#決定木の解釈"><i class="fa fa-check"></i><b>4.4.1</b> 決定木の解釈</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree.html"><a href="tree.html#例-2"><i class="fa fa-check"></i><b>4.4.2</b> 例</a></li>
<li class="chapter" data-level="4.4.3" data-path="tree.html"><a href="tree.html#長所-2"><i class="fa fa-check"></i><b>4.4.3</b> 長所</a></li>
<li class="chapter" data-level="4.4.4" data-path="tree.html"><a href="tree.html#短所-2"><i class="fa fa-check"></i><b>4.4.4</b> 短所</a></li>
<li class="chapter" data-level="4.4.5" data-path="tree.html"><a href="tree.html#ソフトウェア-2"><i class="fa fa-check"></i><b>4.4.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>4.5</b> 決定規則</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rules.html"><a href="rules.html#単一の特徴量による規則学習-oner"><i class="fa fa-check"></i><b>4.5.1</b> 単一の特徴量による規則学習 (OneR)</a></li>
<li class="chapter" data-level="4.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>4.5.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="4.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>4.5.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="4.5.4" data-path="rules.html"><a href="rules.html#長所-3"><i class="fa fa-check"></i><b>4.5.4</b> 長所</a></li>
<li class="chapter" data-level="4.5.5" data-path="rules.html"><a href="rules.html#短所-3"><i class="fa fa-check"></i><b>4.5.5</b> 短所</a></li>
<li class="chapter" data-level="4.5.6" data-path="rules.html"><a href="rules.html#ソフトウェアと代替手法"><i class="fa fa-check"></i><b>4.5.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.6</b> RuleFit</a><ul>
<li class="chapter" data-level="4.6.1" data-path="rulefit.html"><a href="rulefit.html#解釈と例"><i class="fa fa-check"></i><b>4.6.1</b> 解釈と例</a></li>
<li class="chapter" data-level="4.6.2" data-path="rulefit.html"><a href="rulefit.html#理論-1"><i class="fa fa-check"></i><b>4.6.2</b> 理論</a></li>
<li class="chapter" data-level="4.6.3" data-path="rulefit.html"><a href="rulefit.html#長所-4"><i class="fa fa-check"></i><b>4.6.3</b> 長所</a></li>
<li class="chapter" data-level="4.6.4" data-path="rulefit.html"><a href="rulefit.html#短所-4"><i class="fa fa-check"></i><b>4.6.4</b> 短所</a></li>
<li class="chapter" data-level="4.6.5" data-path="rulefit.html"><a href="rulefit.html#ソフトウェアと代替手法-1"><i class="fa fa-check"></i><b>4.6.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>4.7</b> その他の解釈可能なモデル</a><ul>
<li class="chapter" data-level="4.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#単純ベイズ分類器-naive-bayes-classifier"><i class="fa fa-check"></i><b>4.7.1</b> 単純ベイズ分類器 (Naive Bayes Classifier)</a></li>
<li class="chapter" data-level="4.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k近傍法"><i class="fa fa-check"></i><b>4.7.2</b> k近傍法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> モデル非依存(Model-Agnostic)な手法</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#例-3"><i class="fa fa-check"></i><b>5.1.1</b> 例</a></li>
<li class="chapter" data-level="5.1.2" data-path="pdp.html"><a href="pdp.html#長所-5"><i class="fa fa-check"></i><b>5.1.2</b> 長所</a></li>
<li class="chapter" data-level="5.1.3" data-path="pdp.html"><a href="pdp.html#短所-5"><i class="fa fa-check"></i><b>5.1.3</b> 短所</a></li>
<li class="chapter" data-level="5.1.4" data-path="pdp.html"><a href="pdp.html#ソフトウェアと代替手法-2"><i class="fa fa-check"></i><b>5.1.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ice.html"><a href="ice.html#例-4"><i class="fa fa-check"></i><b>5.2.1</b> 例</a></li>
<li class="chapter" data-level="5.2.2" data-path="ice.html"><a href="ice.html#長所-6"><i class="fa fa-check"></i><b>5.2.2</b> 長所</a></li>
<li class="chapter" data-level="5.2.3" data-path="ice.html"><a href="ice.html#短所-6"><i class="fa fa-check"></i><b>5.2.3</b> 短所</a></li>
<li class="chapter" data-level="5.2.4" data-path="ice.html"><a href="ice.html#ソフトウェアと代替手法-3"><i class="fa fa-check"></i><b>5.2.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5.3</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ale.html"><a href="ale.html#モチベーションと直感"><i class="fa fa-check"></i><b>5.3.1</b> モチベーションと直感</a></li>
<li class="chapter" data-level="5.3.2" data-path="ale.html"><a href="ale.html#理論-2"><i class="fa fa-check"></i><b>5.3.2</b> 理論</a></li>
<li class="chapter" data-level="5.3.3" data-path="ale.html"><a href="ale.html#予測"><i class="fa fa-check"></i><b>5.3.3</b> 予測</a></li>
<li class="chapter" data-level="5.3.4" data-path="ale.html"><a href="ale.html#例-6"><i class="fa fa-check"></i><b>5.3.4</b> 例</a></li>
<li class="chapter" data-level="5.3.5" data-path="ale.html"><a href="ale.html#利点"><i class="fa fa-check"></i><b>5.3.5</b> 利点</a></li>
<li class="chapter" data-level="5.3.6" data-path="ale.html"><a href="ale.html#欠点"><i class="fa fa-check"></i><b>5.3.6</b> 欠点</a></li>
<li class="chapter" data-level="5.3.7" data-path="ale.html"><a href="ale.html#実装と代替手法"><i class="fa fa-check"></i><b>5.3.7</b> 実装と代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.4</b> 特徴量の相互作用</a><ul>
<li class="chapter" data-level="5.4.1" data-path="interaction.html"><a href="interaction.html#特徴量の相互作用とは"><i class="fa fa-check"></i><b>5.4.1</b> 特徴量の相互作用とは</a></li>
<li class="chapter" data-level="5.4.2" data-path="interaction.html"><a href="interaction.html#friedman-の-h統計量の理論"><i class="fa fa-check"></i><b>5.4.2</b> Friedman の H統計量の理論</a></li>
<li class="chapter" data-level="5.4.3" data-path="interaction.html"><a href="interaction.html#例-7"><i class="fa fa-check"></i><b>5.4.3</b> 例</a></li>
<li class="chapter" data-level="5.4.4" data-path="interaction.html"><a href="interaction.html#利点-1"><i class="fa fa-check"></i><b>5.4.4</b> 利点</a></li>
<li class="chapter" data-level="5.4.5" data-path="interaction.html"><a href="interaction.html#欠点-1"><i class="fa fa-check"></i><b>5.4.5</b> 欠点</a></li>
<li class="chapter" data-level="5.4.6" data-path="interaction.html"><a href="interaction.html#実装"><i class="fa fa-check"></i><b>5.4.6</b> 実装</a></li>
<li class="chapter" data-level="5.4.7" data-path="interaction.html"><a href="interaction.html#代替手法"><i class="fa fa-check"></i><b>5.4.7</b> 代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.5</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="5.5.1" data-path="feature-importance.html"><a href="feature-importance.html#理論-3"><i class="fa fa-check"></i><b>5.5.1</b> 理論</a></li>
<li class="chapter" data-level="5.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>5.5.2</b> 特徴量の重要度は、学習データとテストデータのどちらで計算するべきか</a></li>
<li class="chapter" data-level="5.5.3" data-path="feature-importance.html"><a href="feature-importance.html#例と解釈"><i class="fa fa-check"></i><b>5.5.3</b> 例と解釈</a></li>
<li class="chapter" data-level="5.5.4" data-path="feature-importance.html"><a href="feature-importance.html#利点-2"><i class="fa fa-check"></i><b>5.5.4</b> 利点</a></li>
<li class="chapter" data-level="5.5.5" data-path="feature-importance.html"><a href="feature-importance.html#欠点-2"><i class="fa fa-check"></i><b>5.5.5</b> 欠点</a></li>
<li class="chapter" data-level="5.5.6" data-path="feature-importance.html"><a href="feature-importance.html#ソフトウェアと代替手法-4"><i class="fa fa-check"></i><b>5.5.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.6</b> グローバルサロゲート (Global Surrogate)</a><ul>
<li class="chapter" data-level="5.6.1" data-path="global.html"><a href="global.html#理論-4"><i class="fa fa-check"></i><b>5.6.1</b> 理論</a></li>
<li class="chapter" data-level="5.6.2" data-path="global.html"><a href="global.html#例-8"><i class="fa fa-check"></i><b>5.6.2</b> 例</a></li>
<li class="chapter" data-level="5.6.3" data-path="global.html"><a href="global.html#長所-7"><i class="fa fa-check"></i><b>5.6.3</b> 長所</a></li>
<li class="chapter" data-level="5.6.4" data-path="global.html"><a href="global.html#短所-7"><i class="fa fa-check"></i><b>5.6.4</b> 短所</a></li>
<li class="chapter" data-level="5.6.5" data-path="global.html"><a href="global.html#ソフトウェア-3"><i class="fa fa-check"></i><b>5.6.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.7</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="5.7.1" data-path="lime.html"><a href="lime.html#表形式データにおける-lime"><i class="fa fa-check"></i><b>5.7.1</b> 表形式データにおける LIME</a></li>
<li class="chapter" data-level="5.7.2" data-path="lime.html"><a href="lime.html#テキストデータに対するlime"><i class="fa fa-check"></i><b>5.7.2</b> テキストデータに対するLIME</a></li>
<li class="chapter" data-level="5.7.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.7.3</b> 画像データに対するLIME</a></li>
<li class="chapter" data-level="5.7.4" data-path="lime.html"><a href="lime.html#長所-8"><i class="fa fa-check"></i><b>5.7.4</b> 長所</a></li>
<li class="chapter" data-level="5.7.5" data-path="lime.html"><a href="lime.html#短所-8"><i class="fa fa-check"></i><b>5.7.5</b> 短所</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>5.8</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="5.8.1" data-path="anchors.html"><a href="anchors.html#anchor-の発見"><i class="fa fa-check"></i><b>5.8.1</b> Anchor の発見</a></li>
<li class="chapter" data-level="5.8.2" data-path="anchors.html"><a href="anchors.html#複雑性と実行時間"><i class="fa fa-check"></i><b>5.8.2</b> 複雑性と実行時間</a></li>
<li class="chapter" data-level="5.8.3" data-path="anchors.html"><a href="anchors.html#表形式データの例"><i class="fa fa-check"></i><b>5.8.3</b> 表形式データの例</a></li>
<li class="chapter" data-level="5.8.4" data-path="anchors.html"><a href="anchors.html#長所-9"><i class="fa fa-check"></i><b>5.8.4</b> 長所</a></li>
<li class="chapter" data-level="5.8.5" data-path="anchors.html"><a href="anchors.html#短所-9"><i class="fa fa-check"></i><b>5.8.5</b> 短所</a></li>
<li class="chapter" data-level="5.8.6" data-path="anchors.html"><a href="anchors.html#ソフトウェアと代替手法-5"><i class="fa fa-check"></i><b>5.8.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.9</b> シャープレイ値 (Shapley Values)</a><ul>
<li class="chapter" data-level="5.9.1" data-path="shapley.html"><a href="shapley.html#一般的なアイデア"><i class="fa fa-check"></i><b>5.9.1</b> 一般的なアイデア</a></li>
<li class="chapter" data-level="5.9.2" data-path="shapley.html"><a href="shapley.html#例と解釈-1"><i class="fa fa-check"></i><b>5.9.2</b> 例と解釈</a></li>
<li class="chapter" data-level="5.9.3" data-path="shapley.html"><a href="shapley.html#シャープレイ値の詳細"><i class="fa fa-check"></i><b>5.9.3</b> シャープレイ値の詳細</a></li>
<li class="chapter" data-level="5.9.4" data-path="shapley.html"><a href="shapley.html#長所-10"><i class="fa fa-check"></i><b>5.9.4</b> 長所</a></li>
<li class="chapter" data-level="5.9.5" data-path="shapley.html"><a href="shapley.html#短所-10"><i class="fa fa-check"></i><b>5.9.5</b> 短所</a></li>
<li class="chapter" data-level="5.9.6" data-path="shapley.html"><a href="shapley.html#ソフトウェアと代替手法-6"><i class="fa fa-check"></i><b>5.9.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>5.10</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="5.10.1" data-path="shap.html"><a href="shap.html#定義"><i class="fa fa-check"></i><b>5.10.1</b> 定義</a></li>
<li class="chapter" data-level="5.10.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>5.10.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="5.10.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>5.10.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="5.10.4" data-path="shap.html"><a href="shap.html#例-12"><i class="fa fa-check"></i><b>5.10.4</b> 例</a></li>
<li class="chapter" data-level="5.10.5" data-path="shap.html"><a href="shap.html#shap-特徴量重要度-shap-feature-importance"><i class="fa fa-check"></i><b>5.10.5</b> SHAP 特徴量重要度 (SHAP Feature Importance)</a></li>
<li class="chapter" data-level="5.10.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>5.10.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="5.10.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>5.10.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="5.10.8" data-path="shap.html"><a href="shap.html#shap-相互作用値-shap-interaction-values"><i class="fa fa-check"></i><b>5.10.8</b> SHAP 相互作用値 (SHAP Interaction Values)</a></li>
<li class="chapter" data-level="5.10.9" data-path="shap.html"><a href="shap.html#clustering-shap-values"><i class="fa fa-check"></i><b>5.10.9</b> Clustering SHAP values</a></li>
<li class="chapter" data-level="5.10.10" data-path="shap.html"><a href="shap.html#長所-11"><i class="fa fa-check"></i><b>5.10.10</b> 長所</a></li>
<li class="chapter" data-level="5.10.11" data-path="shap.html"><a href="shap.html#短所-11"><i class="fa fa-check"></i><b>5.10.11</b> 短所</a></li>
<li class="chapter" data-level="5.10.12" data-path="shap.html"><a href="shap.html#ソフトウェア-4"><i class="fa fa-check"></i><b>5.10.12</b> ソフトウェア</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>6</b> 例示に基づいた説明手法</a><ul>
<li class="chapter" data-level="6.1" data-path="反事実的.html"><a href="反事実的.html"><i class="fa fa-check"></i><b>6.1</b> 反事実的説明 (Counterfactual Explanations)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="反事実的.html"><a href="反事実的.html#反事実的説明の生成"><i class="fa fa-check"></i><b>6.1.1</b> 反事実的説明の生成</a></li>
<li class="chapter" data-level="6.1.2" data-path="反事実的.html"><a href="反事実的.html#例-13"><i class="fa fa-check"></i><b>6.1.2</b> 例</a></li>
<li class="chapter" data-level="6.1.3" data-path="反事実的.html"><a href="反事実的.html#長所-12"><i class="fa fa-check"></i><b>6.1.3</b> 長所</a></li>
<li class="chapter" data-level="6.1.4" data-path="反事実的.html"><a href="反事実的.html#短所-12"><i class="fa fa-check"></i><b>6.1.4</b> 短所</a></li>
<li class="chapter" data-level="6.1.5" data-path="反事実的.html"><a href="反事実的.html#ソフトウェアと代替手法-7"><i class="fa fa-check"></i><b>6.1.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>6.2</b> 敵対的サンプル (Adversarial Examples)</a><ul>
<li class="chapter" data-level="6.2.1" data-path="adversarial.html"><a href="adversarial.html#手法及び例"><i class="fa fa-check"></i><b>6.2.1</b> 手法及び例</a></li>
<li class="chapter" data-level="6.2.2" data-path="adversarial.html"><a href="adversarial.html#サイバーセキュリティーの観点"><i class="fa fa-check"></i><b>6.2.2</b> サイバーセキュリティーの観点</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>6.3</b> prototype と criticism</a><ul>
<li class="chapter" data-level="6.3.1" data-path="proto.html"><a href="proto.html#理論-5"><i class="fa fa-check"></i><b>6.3.1</b> 理論</a></li>
<li class="chapter" data-level="6.3.2" data-path="proto.html"><a href="proto.html#例-14"><i class="fa fa-check"></i><b>6.3.2</b> 例</a></li>
<li class="chapter" data-level="6.3.3" data-path="proto.html"><a href="proto.html#長所-13"><i class="fa fa-check"></i><b>6.3.3</b> 長所</a></li>
<li class="chapter" data-level="6.3.4" data-path="proto.html"><a href="proto.html#短所-13"><i class="fa fa-check"></i><b>6.3.4</b> 短所</a></li>
<li class="chapter" data-level="6.3.5" data-path="proto.html"><a href="proto.html#コードと代替手法"><i class="fa fa-check"></i><b>6.3.5</b> コードと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>6.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="6.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>6.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="6.4.2" data-path="influential.html"><a href="influential.html#影響関数-influence-functions"><i class="fa fa-check"></i><b>6.4.2</b> 影響関数 (Influence Functions)</a></li>
<li class="chapter" data-level="6.4.3" data-path="influential.html"><a href="influential.html#長所-14"><i class="fa fa-check"></i><b>6.4.3</b> 長所</a></li>
<li class="chapter" data-level="6.4.4" data-path="influential.html"><a href="influential.html#短所-14"><i class="fa fa-check"></i><b>6.4.4</b> 短所</a></li>
<li class="chapter" data-level="6.4.5" data-path="influential.html"><a href="influential.html#ソフトウェアと代替手法-8"><i class="fa fa-check"></i><b>6.4.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> ニューラルネットワークの解釈</a><ul>
<li class="chapter" data-level="7.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>7.1</b> 学習された特徴量</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cnn-features.html"><a href="cnn-features.html#特徴量の可視化"><i class="fa fa-check"></i><b>7.1.1</b> 特徴量の可視化</a></li>
<li class="chapter" data-level="7.1.2" data-path="cnn-features.html"><a href="cnn-features.html#ネットワークの解剖"><i class="fa fa-check"></i><b>7.1.2</b> ネットワークの解剖</a></li>
<li class="chapter" data-level="7.1.3" data-path="cnn-features.html"><a href="cnn-features.html#利点-3"><i class="fa fa-check"></i><b>7.1.3</b> 利点</a></li>
<li class="chapter" data-level="7.1.4" data-path="cnn-features.html"><a href="cnn-features.html#欠点-3"><i class="fa fa-check"></i><b>7.1.4</b> 欠点</a></li>
<li class="chapter" data-level="7.1.5" data-path="cnn-features.html"><a href="cnn-features.html#ソフトウェアとその他の資料"><i class="fa fa-check"></i><b>7.1.5</b> ソフトウェアとその他の資料</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> 解釈可能な機械学習の未来</a><ul>
<li class="chapter" data-level="8.1" data-path="機械学習の未来.html"><a href="機械学習の未来.html"><i class="fa fa-check"></i><b>8.1</b> 機械学習の未来</a></li>
<li class="chapter" data-level="8.2" data-path="解釈性の未来.html"><a href="解釈性の未来.html"><i class="fa fa-check"></i><b>8.2</b> 解釈性の未来</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> 著者貢献</a></li>
<li class="chapter" data-level="10" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>10</b> この本の引用</a></li>
<li class="chapter" data-level="11" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>11</b> 翻訳</a></li>
<li class="chapter" data-level="12" data-path="謝辞.html"><a href="謝辞.html"><i class="fa fa-check"></i><b>12</b> 謝辞</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used-for-examples.html"><a href="r-packages-used-for-examples.html"><i class="fa fa-check"></i>R Packages Used for Examples</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="proto" class="section level2">
<h2><span class="header-section-number">6.3</span> prototype と criticism</h2>
<!--## Prototypes and Criticisms {#proto}-->
<!--
A **prototype** is a data instance that is representative of all the data.
A **criticism** is a data instance that is not well represented by the set of prototypes.
The purpose of criticisms is to provide insights together with prototypes, especially for data points which the prototypes do not represent well.
Prototypes and criticisms can be used independently from a machine learning model to describe the data, but they can also be used to create an interpretable model or to make a black box model interpretable.

In this chapter I use the expression "data point" to refer to a single instance, to emphasize the interpretation that an instance is also a point in a coordinate system where each feature is a dimension.
The following figure shows a simulated data distribution, with some of the instances chosen as prototypes and some as criticisms.
The small points are the data, the large points the criticisms and the large squares the prototypes.
The prototypes are selected (manually) to cover the centers of the data distribution and the criticisms are points in a cluster without a prototype.
Prototypes and criticisms are always actual instances from the data.
-->
<p><strong>prototype</strong> は、すべてのデータの代表であるデータインスタンスです。 <strong>criticism</strong> は prototype の集まりではうまく表現できないデータインスタンスです。 criticism の目的は、特に、prototype が良く表現できないデータ点について、prototype とともに見識を提供することです。 prototype と criticism は、データを記述するのに機械学習モデルとは独立に使用可能ですが、解釈可能なモデルを作成したり、ブラックボックスモデルを解釈可能にするために使用できます。</p>
<p>この章では、1つのインスタンスを指すときや、あるインスタンスはそれぞれの特徴量の次元である座標系における1つの点でもあることの説明を強調するために &quot;データ点&quot; という表現を使います。 次の図は、シミュレーションによるデータの分布を示していて、prototype として、または criticism として選択されたインスタンスもいくつかあります。 小さな点はデータ、大きな点は criticism で、大きな四角形は prototype です。 prototype はデータ分布の中央を覆うように(手動で)選択されていて、criticism は prototype を除くクラスタ内の点です。 prototype と criticism は常に実際のデータに含まれるインスタンスです。</p>
<!--
fig.cap = "Prototypes and criticisms for a data distribution with two features x1 and x2."
-->
<div class="figure"><span id="fig:unnamed-chunk-43"></span>
<img src="images/unnamed-chunk-43-1.png" alt="2つの特徴量 x1 と x2 を持つデータ分布の prototype と criticism。" width="1050" />
<p class="caption">
FIGURE 6.6: 2つの特徴量 x1 と x2 を持つデータ分布の prototype と criticism。
</p>
</div>
<!--
I selected the prototypes manually, which does not scale well and probably leads to poor results.
There are many approaches to find prototypes in the data.
One of these is k-medoids, a clustering algorithm related to the k-means algorithm.
Any clustering algorithm that returns actual data points as cluster centers would qualify for selecting prototypes.
But most of these methods find only prototypes, but no criticisms.
This chapter presents MMD-critic by Kim et. al (2016)[^critique], an approach that combines prototypes and criticisms in a single framework.
-->
<p>手作業で prototype を選びましたが、これは上手くスケールせず悪い結果になりそうです。 データの中から prototype を見つけるためのアプローチはたくさんあります。 そのうちの1つが k-medoids であり、k-means アルゴリズムに関連するクラスタリングアルゴリズムです。 クラスタの中心として実際のデータ点を返すようなクラスタリングアルゴリズムであれば prototype の選択に適しているでしょう。 しかし、それらの手法の殆どは prototype しか見つけることができず、criticism は見つけられません。 この章では、Kim et.al (2016)<a href="#fn61" class="footnoteRef" id="fnref61"><sup>61</sup></a> による MMD-critic という、prototype と criticism を1つのフレームワークに組み合わせたアプローチを紹介します。</p>
<!--
MMD-critic compares the distribution of the data and the distribution of the selected prototypes.
This is the central concept for understanding the MMD-critic method.
MMD-critic selects prototypes that minimize the discrepancy between the two distributions.
Data points in areas with high density are good prototypes, especially when points are selected from different "data clusters".
Data points from regions that are not well explained  by the prototypes are selected as criticisms.

Let us delve deeper into the theory.
-->
<p>MMD-critic は、データの分布と選択された prototype の分布を比較します。 これが MMD-critic を理解するための中心的な概念です。 MMD-critic は、2つの分布間の差異を最小化するような prototype を選択します。 密度の高い領域のデータ点、特に、異なる&quot;データクラスタ&quot;から選ばれたとき、良い prototype となります。 prototype ではうまく説明できない範囲から得られるデータ点は criticism として選択されます。</p>
<p>理論を深掘りしましょう。</p>
<!--
### Theory
-->
<div id="理論-5" class="section level3">
<h3><span class="header-section-number">6.3.1</span> 理論</h3>
<!--
The MMD-critic procedure on a high-level can be summarized briefly:

1. Select the number of prototypes and criticisms you want to find.
1. Find prototypes with greedy search.
Prototypes are selected so that the distribution of the prototypes is close to the data distribution.
1. Find criticisms with greedy search.
Points are selected as criticisms where the distribution of prototypes differs from the distribution of the data.
-->
<p>高レベルでの MMD-critic の手順は以下のように簡潔にまとめられます。</p>
<ol style="list-style-type: decimal">
<li>見つけたい prototype と criticism の数を選びます。</li>
<li>prototype を貪欲法で探索します。 prototype の分布がデータの分布と近くなるように prototype は選択されます。</li>
<li>criticism を貪欲法で探索します。 prototype の分布とデータの分布が異なる点は criticism として選択されます。</li>
</ol>
<!--
We need a couple of ingredients to find prototypes and criticisms for a dataset with MMD-critic.
As the most basic ingredient, we need a **kernel function** to estimate the data densities.
A kernel is a function that weighs two data points according to their proximity.
Based on density estimates, we need a measure that tells us how different two distributions are so that we can determine whether the distribution of the prototypes we select is close to the data distribution.
This is solved by measuring the **maximum mean discrepancy (MMD)**.
Also based on the kernel function, we need the **witness function** to tell us how different two distributions are at a particular data point.
With the witness function, we can select criticisms, i.e. data points at which the distribution of prototypes and data diverges and the witness function takes on large absolute values.
The last ingredient is a search strategy for good prototypes and criticisms, which is solved with a simple **greedy search**.
-->
<p>データセットにおける prototype と criticism を MMD-critic によって見つけるために必要な構成要素がいくつかあります。 最も基本的な要素として、データの密度を推定するために<strong>カーネル関数</strong>が必要です。 カーネルは2つのデータ点の近さに応じて重み付けする関数です。 密度推定に基づいて、選択された prototype の分布がデータの分布と近いかどうか知るために、2つの分布がどれだけ異なるのかを知る基準が必要です。 これは、 <strong>maximum mean discrepancy(MMD)</strong> を測ることで解決します。 また、カーネル関数に基づいて、特定のデータ点において2つの分布がどれだけ異なるのか知るための <strong>witness 関数</strong>が必要です。 prototype とデータの分布が乖離していて、witness 関数が大きな絶対値をとるデータ点を criticism として選択できます。 最後の要素は、良い prototype と criticism のための探索戦略であり、単に<strong>貪欲探索</strong>によって解決します。</p>
<!--
Let us start with the **maximum mean discrepancy (MMD)**, which measures the discrepancy between two distributions.
The selection of prototypes creates a density distribution of prototypes.
We want to evaluate whether the prototypes distribution differs from the data distribution.
We estimate both with kernel density functions.
The maximum mean discrepancy measures the difference between two distributions, which is the supremum over a function space of differences between the expectations according to the two distributions.
All clear?
Personally, I understand these concepts much better when I see how something is calculated with data.
The following formula shows how to calculate the squared MMD measure (MMD2):
-->
<p>2つの分布の違いを測る <strong>maximum mean discrepancy(MMD)</strong> から初めていきましょう。 prototype の選択によって prototype の密度分布を作ります。 我々は prototype の分布がデータの分布と異なるのかどうか評価したいです。 カーネル密度関数によってその両方を推定します。 maximum mean discrepancy は2つの分布間の差異を測る指標です。これは2つの分布に対応する期待値間の差異を表す関数空間上での上界です。 わかるでしょうか。 個人的に、これらの概念は、データからどんなものが計算されるのかを見た方が遥かに良く理解できます。 次の式は、MMD の二乗(MMD2)の計算方法を示しています。</p>
<p><span class="math display">\[MMD^2=\frac{1}{m^2}\sum_{i,j=1}^m{}k(z_i,z_j)-\frac{2}{mn}\sum_{i,j=1}^{m,n}k(z_i,x_j)+\frac{1}{n^2}\sum_{i,j=1}^n{}k(x_i,x_j)\]</span></p>
<!--
k is a kernel function that measures the similarity of two points, but more about this later.
m is the number of prototypes z, and n is the number of data points x in our original dataset.
The prototypes z are a selection of data points x.
Each point is multidimensional, that is it can have multiple features.
The goal of MMD-critic is to minimize MMD2.
The closer MMD2 is to zero, the better the distribution of the prototypes fits the data.
The key to bringing MMD2 down to zero is the term in the middle, which calculates the average proximity between the prototypes and all other data points (multiplied by 2).
If this term adds up to the first term (the average proximity of the prototypes to each other) plus the last term (the average proximity of the data points to each other), then the prototypes explain the data perfectly.
Try out what would happen to the formula if you used all n data points as prototypes.
-->
<p>k は2点の類似度を測るカーネル関数ですが、これについては後述します。 m は prototype である z の数で、n は元々のデータセット上のデータ点 x の数です。 prototype の z はデータ点 x から選ばれたものです。 それぞれの点は多次元なので、複数の特徴量を持つことができます。 MMD-critic の目的は MMD2 を最小化することです。 MMD2 がゼロに近いほど、prototype の分布はデータによくフィットします。 MMD2 をゼロに近づける鍵となるのは中央の項で、これは prototype と全てのデータ点との平均的な（2乗された）近さを計算します。 この項と、最初の項（prototype どうしの平均的な近さ）に加え、最後の項（データどうしの平均的な近さ）を足し上げれば、prototype はデータを完璧に説明できます。 n 個全てのデータ点を prototype として使った時、この式に何がどうなるのか試してみてください。</p>
<!--
The following graphic illustrates the MMD2 measure.
The first plot shows the data points with two features, whereby the estimation of the data density is displayed with a shaded background.
Each of the other plots shows different selections of prototypes, along with the MMD2 measure in the plot titles.
The prototypes are the large dots and their distribution is shown as contour lines.
The selection of the prototypes that best covers the data in these scenarios (bottom left) has the lowest discrepancy value.
-->
<p>次の図は、MMD2 measure を表現しています。 最初の図は、データ点を2つの特徴量で示していて、推定データ密度を背景にグラデーションで表現しています。 その他はそれぞれ異なる prototype の選択をしていて、その時の MMD2 measure を図の上部に記しています。 prototype は大きなドットで、その分布は等高線で表しています。 これらの中で最もデータを覆っている prototype の選択は（左下）最も低い乖離度になっています。</p>
<!--
fig.cap = "The squared maximum mean discrepancy measure (MMD2) for a dataset with two features and different selections of prototypes."
-->
<div class="figure"><span id="fig:mmd"></span>
<img src="images/mmd-1.png" alt="二つの特徴量を持つデータセットに対する異なる prototype の MMD の二乗(MMD2)" width="1050" />
<p class="caption">
FIGURE 6.7: 二つの特徴量を持つデータセットに対する異なる prototype の MMD の二乗(MMD2)
</p>
</div>
<!--
A choice for the kernel is the radial basis function kernel:
-->
<p>カーネルの選択は動径基底関数カーネルです。</p>
<p><span class="math display">\[k(x,x^\prime)=exp\left(-\gamma||x-x^\prime||^2\right)\]</span></p>
<!--
where ||x-x'||^2^ is the Euclidean distance between two points and $\gamma$ is a scaling parameter.
The value of the kernel decreases with the distance between the two points and ranges between zero and one:
Zero when the two points are infinitely far apart;
one when the two points are equal.
-->
<p>ここで、||x-x'||<sup>2</sup> は2点間のユークリッド距離で <span class="math inline">\(\gamma\)</span> はスケールパラメータです。 カーネルの値は2点間の距離に応じて減少し、0と1の範囲に収まります。 2点間が無限遠にある時に 0 になり、同じ点にある時 1 になります。</p>
<!--
We combine the MMD2 measure, the kernel and greedy search in an algorithm for finding prototypes:
-->
<p>prototype を見つけるためのアルゴリズムとして MMD2、カーネル、貪欲法を組み合わせます。</p>
<!--
- Start with an empty list of prototypes.
- While the number of prototypes is below the chosen number m:
    - For each point in the dataset, check how much MMD2 is reduced when the point is added to the list of prototypes. Add the data point that minimizes the MMD2 to the list.
- Return the list of prototypes.

The remaining ingredient for finding criticisms is the witness function, which tells us how much two density estimates differ at a particular point.
It can be estimated using:
-->
<ul>
<li>prototype の空のリストを用意します。</li>
<li>prototype の数は選ばれた m 個以下として、
<ul>
<li>データセット上のそれぞれの点が prototype のリストに追加された時に MMD2 がどのくらい減少するのか確認します。MMD2 を最小化するようなデータ点をリストに追加します。</li>
</ul></li>
<li>prototype のリストを返します。</li>
</ul>
<p>criticism を探すための残りの成分は、ある点での2つの推定密度がどのくらい異なるのか知るための witness 関数になります。 これは次のように推定できます。</p>
<p><span class="math display">\[witness(x)=\frac{1}{n}\sum_{i=1}^nk(x,x_i)-\frac{1}{m}\sum_{j=1}^mk(x,z_j)\]</span></p>
<!--
For two datasets (with the same features), the witness function gives you the means of evaluating in which empirical distribution the point x fits better.
To find criticisms, we look for extreme values of the witness function in both negative and positive directions.
The first term in the witness function is the average proximity between point x and the data, and, respectively, the second term is the average proximity between point x and the prototypes.
If the witness function for a point x is close to zero, the density function of the data and the prototypes are close together, which means that the distribution of prototypes resembles the distribution of the data at point x.
A negative witness function at point x means that the prototype distribution overestimates the data distribution (for example if we select a prototype but there are only few data points nearby);
a positive witness function at point x means that the prototype distribution underestimates the data distribution (for example if there are many data points around x but we have not selected any prototypes nearby).
-->
<p>2つの同じ特徴量を持つデータセットがある時、witness関数は、点 x がどちらの経験的分布に良く適合するかを評価する手段となります。 criticism を見つけるために、witness 関数の外れ値を正負どちらの方向についても探します。 witness 関数の最初の項は点 x とデータの類似度の平均で、同様に、2番目の項はデータ点と prototype 間の類似度の平均です。 ある点 x での witness 関数が 0 に近づくと、データとプロトタイプの密度関数は互いに近づきます。これはプロトタイプの分布が点 x でのデータの分布と類似することです。 witness 関数が点 x で負であるとき、prototype の分布はデータの分布を過大評価しています(例えば、prototype を1つ選んだのに近くにデータ点が少ししかない時など)。 一方、正のとき、プロトタイプの分布はデータの分布を過小評価しています(例えば、x の周りのデータ点が沢山あるのに、近くの prototype を1つも選ばないなど)。</p>
<!--
To give you more intuition, let us reuse the prototypes from the plot beforehand with the lowest MMD2 and display the witness function for a few manually selected points.
The labels in the following plot show the value of the witness function for various points marked as triangles.
Only the point in the middle has a high absolute value and is therefore a good candidate for a criticism.
-->
<p>より直感的に理解するために、先ほどプロットした prototype のうち最も小さな MMD2 をもつものを再利用して、いくつか手動で選んだ点に対する witness 関数を表示してみましょう。 次の図のラベルは、三角形で表された様々な点に関する witness 関数の値を示しています。 中央の点のみが大きな絶対値をもつことから、criticism として良い候補になります。</p>
<!--
fig.cap = "Evaluations of the witness function at different points."
-->
<div class="figure"><span id="fig:witness"></span>
<img src="images/witness-1.png" alt="異なる点における witness 関数の評価" width="1050" />
<p class="caption">
FIGURE 6.8: 異なる点における witness 関数の評価
</p>
</div>
<!--
The witness function allows us to explicitly search for data instances that are not well represented by the prototypes.
Criticisms are points with high absolute value in the witness function.
Like prototypes, criticisms are also found through greedy search.
But instead of reducing the overall MMD2, we are looking for points that maximize a cost function that includes the witness function and a regularizer term.
The additional term in the optimization function enforces diversity in the points, which is needed so that the points come from different clusters.

This second step is independent of how the prototypes are found.
I could also have handpicked some prototypes and used the procedure described here to learn criticisms.
Or the prototypes could come from any clustering procedure, like k-medoids.
-->
<p>witness 関数を使うことで、prototype によって上手く表されていないデータインスタンスを明示的に探すことができます。 criticism は witness 関数の出力の中で、高い絶対値を持つ点のことを言います。 prototype と同様に、criticism も貪欲法により探索されます。 しかし、全体の MMD2 を減らすのではなく、witness 関数と正則化項を含むコスト関数を最大化する点が探されます。 正則化項は、異なるクラスターから点が選択されるよう、点の多様性を強制します。</p>
<p>この2つ目のステップは、prototype の決定方法とは独立しています。 criticism を学ぶため、ここで述べた手順といくつかの prototype を選んでみます。 もしくは、prototype は、例えば k-medoids のようなクラスタリング手法により選択できます。</p>
<!--
That is it with the important parts of MMD-critic theory.
One question remains:
**How can MMD-critic be used for interpretable machine learning?**

MMD-critic can add interpretability in three ways:
By helping to better understand the data distribution;
by building an interpretable model;
by making a black box model interpretable.

If you apply MMD-critic to your data to find prototypes and criticisms, it will improve your understanding of the data, especially if you have a complex data distribution with edge cases.
But with MMD-critic you can achieve more!
-->
<p>これで、MMD-critic 理論の重要な部分は以上です。 ただし、1つ疑問が残されています。 <strong>機械学習の解釈性に、MMD-criticはどのように使われるのか</strong>ということです。</p>
<p>MMD-critic は、1) データの分布の理解に役立つ、2) 解釈可能なモデルを構築する、3) ブラックボックスを解釈可能にする という3つの観点で、解釈性に寄与します。 prototype と criticism を探すために、データセットに MMD-critic を適用すると、データへの理解が深まります。 しかし、MMD-critic を用いると、さらに期待できることがあります。</p>
<!--
For example, you can create an interpretable prediction model: a so-called "nearest prototype model".
The prediction function is defined as:
-->
<p>例えば、解釈可能な予測モデルを作ることもできます。 これを&quot;nearest prototype model&quot;と呼びましょう。 この予測モデルは以下のように定義されます。</p>
<p><span class="math display">\[\hat{f}(x)=argmax_{i\in{}S}k(x,x_i)\]</span></p>
<!--
which means that we select the prototype i from the set of prototypes S that is closest to the new data point, in the sense that it yields the highest value of the kernel function.
The prototype itself is returned as an explanation for the prediction.
This procedure has three tuning parameters:
The type of kernel, the kernel scaling parameter and the number of prototypes.
-->
<p>このモデルは、カーネル関数の出力が大きいという意味で、新しいデータに最も近い prototype が S から選ばれるという意味になります。 prototype 自体は、予測結果に対する説明として考えられます。 この方法には、カーネルの種類、カーネルのスケールパラメータ、prototypeの数という3つのハイパーパラメータがあります。</p>
<!--
All parameters can be optimized within a cross validation loop.
The criticisms are not used in this approach.

As a third option, we can use MMD-critic to make any machine learning model globally explainable by examining prototypes and criticisms along with their model predictions.
The procedure is as follows:

1. Find prototypes and criticisms with MMD-critic.
1. Train a machine learning model as usual.
1. Predict outcomes for the prototypes and criticisms with the machine learning model.
1. Analyse the predictions: In which cases was the algorithm wrong?
Now you have a number of examples that represent the data well and help you to find the weaknesses of the machine learning model.
-->
<p>全てのパラメータは、クロスバリデーションで最適化できます。 criticism はこのアプローチでは使われません。</p>
<p>3つ目の MMD-critic の使い方として、prototype と criticism により任意の機械学習モデルを大域的に説明可能にできます。 方法としては、次の通りです。</p>
<ol style="list-style-type: decimal">
<li>prototype と criticism を MMD-critic を用いて探索</li>
<li>機械学習モデルを学習</li>
<li>prototype と criticism に対して、学習された機械学習モデルを用いて、予測結果を取得</li>
<li>予測結果を解析: どのケースでモデルは予測を間違えたのか。 これにより、機械学習モデルの弱点を見つけることができ、またデータをよく表現するいくつからのサンプルを見つけることができます。</li>
</ol>
<!--
How does that help?
Remember when Google's image classifier identified black people as gorillas?
Perhaps they should have used the procedure described here before deploying their image recognition model.
It is not enough just to check the performance of the model, because if it were 99% correct, this issue could still be in the 1%.
And labels can also be wrong!
Going through all the training data and performing a sanity check if the prediction is problematic might have revealed the problem, but would be infeasible.
But the selection of -- say a few thousand -- prototypes and criticisms is feasible and could have revealed a problem with the data:
It might have shown that there is a lack of images of people with dark skin, which indicates a problem with the diversity in the dataset.
Or it could have shown one or more images of a person with dark skin as a prototype or (probably) as a criticism with the notorious "gorilla" classification.
I do not promise that MMD-critic would certainly intercept these kind of mistakes, but it is a good sanity check.
-->
<p>これはどのような意味があるでしょうか。 Google の画像分類モデルが、黒人をゴリラとして分類した問題を思い出してください。 おそらく、彼らは、画像認識モデルをデプロイする前に、これらの手法を適用するべきだったのでは無いでしょうか。 99% 正しくても、この問題は残りの 1% にある問題なので、モデルの性能を確認するだけでは、不十分です。 そして、ラベルも間違っていることがあります。 全ての学習データを通して、予測結果に問題が無いか確認していれば、この問題は事前に明らかになったかもしれませんが、それは不可能です。 しかし、数千程度の prototype と ciriticisms を選択と確認し、データに関する問題を明らかにしたでしょう。 肌の黒い人の画像が少なく、つまりデータセットの多様性に問題があることを事前に明らかにできたかもしれません。 あるいは、肌の色が濃い人を prototype として、あるいは悪名高い「ゴリラ」という分類で criticism として、1つ以上の画像を表示していたかもしれません。 MMD-critic が確実にこの類のミスを取り除くとは約束できませんが、よいチェックにはなるでしょう。</p>
<!-- ### Examples -->
</div>
<div id="例-14" class="section level3">
<h3><span class="header-section-number">6.3.2</span> 例</h3>
<!--
The following example of MMD-critic uses a handwritten digit dataset.

Looking at the actual prototypes, you might notice that the number of images per digit is different.
This is because a fixed number of prototypes were searched across the entire dataset and not with a fixed number per class.
As expected, the prototypes show different ways of writing the digits.
-->
<p>MMD-critic の次の例では、手書き数字データセットを使っています。</p>
<p>実際の prototype の見ると、数字ごとの画像の枚数が異なることに気づくかもしれません。 これはクラスごとに固定の数を決めて prototype を選んだ訳ではなく、データセット全体で固定の数を決めて prototype を選んだためです。 予想通り、prototype は各数字の異なる書き方のものが選ばれています。</p>
<!--
fig.cap = "Prototypes for a handwritten digits dataset."
-->
<div class="figure"><span id="fig:prototype-and-criticisms2"></span>
<img src="images/proto-critique2.jpg" alt="手書き数字データセットにおける prototype" width="600" />
<p class="caption">
FIGURE 6.9: 手書き数字データセットにおける prototype
</p>
</div>
<!--
### Advantages
-->
</div>
<div id="長所-13" class="section level3">
<h3><span class="header-section-number">6.3.3</span> 長所</h3>
<!--
In a user study the authors of MMD-critic gave images to the participants, which they had to visually match to one of two sets of images, each representing one of two classes (e.g. two dog breeds).
The **participants performed best when the sets showed prototypes and criticisms** instead of random images of a class.

You are free to **choose the number of prototypes and criticisms**.

MMD-critic works with density estimates of the data.
This **works with any type of data and any type of machine learning model**.

The algorithm is **easy to implement**.
-->
<p>ユーザ研究において、MMD-critic の著者は参加者に画像を与えました。参加者は（犬種など）2つのクラスの1つをそれぞれ表す二枚の画像の内、視覚的にマッチする一枚を選ぶ必要があります。 参加者は、ランダムな画像ではなく、prototype と criticism である画像を与えらた場合に、最高のパフォーマンスを示しました。</p>
<p>prototype と criticism の数は自由に選ぶことができます。 MMD-critic はデータの密度推定にも使えます。 また、MMD-critic はどんなタイプのデータでも、どんなタイプの機械学習モデルにも使うことができます。</p>
<p>アルゴリズムの実装も簡単です。</p>
<!--
MMD-critic is very flexible in the way it is used to increase interpretability.
It can be used to understand complex data distributions.
It can be used to build an interpretable machine learning model.
Or it can shed light on the decision making of a black box machine learning model.

**Finding criticisms is independent of the selection process of the prototypes**.
But it makes sense to select prototypes according to MMD-critic, because then both prototypes and criticisms are created using the same method of comparing prototypes and data densities.
-->
<p>MMD-critic は解釈性を高めるために使う方法において非常に柔軟です。 複雑なデータ分布を理解するために使用できます。 解釈可能な機械学習モデルを作るために使用できます。 あるいは、black box モデルを理解するために役立ちます。</p>
<p><strong>criticisms を見つけることは、prototype を選ぶプロセスとは独立しています</strong>。 しかし、MMD-critic に従って prototype を選ぶことは理にかなっています。なぜなら、prototype も criticisms も、prototype とデータ密度を比較する同じ方法で作れられるためです。</p>
<!--
### Disadvantages
-->
</div>
<div id="短所-13" class="section level3">
<h3><span class="header-section-number">6.3.4</span> 短所</h3>
<!--
While, mathematically, prototypes and criticisms are defined differently, their **distinction is based on a cut-off value** (the number of prototypes).
Suppose you choose a too low number of prototypes to cover the data distribution.
The criticisms would end up in the areas that are not that well explained.
But if you were to add more prototypes they would also end up in the same areas.
Any interpretation has to take into account that criticisms strongly depend on the existing prototypes and the (arbitrary) cut-off value for the number of prototypes.
-->
<p>数学的には、prototype と criticism は異なる定義ですが、これらの違いは、カットオフ値（prototype の数）に基づいています。 データの分布をカバーするために、少ない数の prototype を選択したとします。 この場合、criticisims はうまく説明されていない部分に集中してしまいます。 しかし、より多くの prototype を追加したとしても、同じ範囲に集まってしまうでしょう。 どのような解釈をするにしても、criticisims は prototype の数を決定する（任意の）カットオフ値と既存の prototype に強く依存することを考慮に入れる必要があります。</p>
<!--
You have to **choose the number of prototypes and criticisms**.
As much as this can be nice-to-have, it is also a disadvantage.
How many prototypes and criticisms do we actually need?
The more the better? The less the better?
One solution is to select the number of prototypes and criticisms by measuring how much time humans have for the task of looking at the images, which depends on the particular application.
-->
<p><strong>prototype と criticisims の数を選ぶ必要があります</strong>。 これはメリットにもなりますが、デメリットにもなります。 どの程度の prototype と criticism の数が実際必要でしょうか。 さらに多い方が良いのでしょうか、それとも少ない方が良いのでしょうか。 1つの答えは、人がどれくらいの時間で画像を見ているかを計測することで、prototype と criticism を決める方法です。</p>
<!--
Only when using MMD-critic to build a classifier do we have a way to optimize it directly.
One solution could be a screeplot showing the number of prototypes on the x-axis and the MMD2 measure on the y-axis.
We would choose the number of prototypes where the MMD2 curve flattens.
-->
<p>MMD-critic を用いて分類器を作成する場合、直接最適化する方法があります。 1つの解決手段は、横軸に prototype の数をとり、縦軸に MMD2 の値をプロットして見てみることです。 こうすることで、MMD2 カーブがフラットになったところで、prototype の数を選ぶことができます。</p>
<!--
The other parameters are the choice of the kernel and the kernel scaling parameter.
We have the same problem as with the number of prototypes and criticisms:
**How do we select a kernel and its scaling parameter?**
Again, when we use MMD-critic as a nearest prototype classifier, we can tune the kernel parameters.
For the unsupervised use cases of MMD-critic, however, it is unclear.
(Maybe I am a bit harsh here, since all unsupervised methods have this problem.)
-->
<p>他のパラメータは、カーネルとカーネルのスケールパラメータになります。 ここで、prototype と criticism の数の場合と同様の問題があります。 <strong>どのようにカーネルとカーネルのスケールパラメータを選べば良いでしょうか</strong>。 繰り返しになりますが、MMD-critic を最も近い prototype を返す分類器として使う場合、カーネルパラメータも調整できます。 しかし、MMD-critic の教師なしにおけるユースケースの場合、明らかではありません。 全ての教師なし手法は同様の問題を抱えているため、少し難しいかもしれません。</p>
<!--
It takes all the features as input, **disregarding the fact that some features might not be relevant** for predicting the outcome of interest.
One solution is to use only relevant features, for example image embeddings instead of raw pixels.
This works as long as we have a way to project the original instance onto a representation that contains only relevant information.

There is some code available, but it is **not yet implemented as nicely packaged and documented software**.
-->
<p>これは、すべての特徴量を入力として受け取り、<strong>いくつかの特徴量が関心のある結果を予測するために関連性がないかもしれない</strong>という事実を無視しています。 1つの解決策は、生の画像の代わりに、例えば画像の埋め込みなどの関連する特徴量のみを使うことです。 これは、元のインスタンスを、関連する情報のみを含む表現に射影する方法がある場合のみ有効です。</p>
<p>利用できるコードはありますが、綺麗にパッケージ化され、かつドキュメントがあるものはまだありません。</p>
<!--
### Code and Alternatives
-->
</div>
<div id="コードと代替手法" class="section level3">
<h3><span class="header-section-number">6.3.5</span> コードと代替手法</h3>
<!--
An implementation of MMD-critic can be found here: [https://github.com/BeenKim/MMD-critic](https://github.com/BeenKim/MMD-critic).

The simplest alternative to finding prototypes is [k-medoids](https://en.wikipedia.org/wiki/K-medoids) by Kaufman et. al (1987).[^medoids]

[^medoids]: Kaufman, Leonard, and Peter Rousseeuw. "Clustering by means of medoids". North-Holland (1987).

[^critique]: Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. "Examples are not enough, learn to criticize! Criticism for interpretability." Advances in Neural Information Processing Systems (2016).
-->
<p>MMD-critic の実装は以下にあります。 <a href="https://github.com/BeenKim/MMD-critic" class="uri">https://github.com/BeenKim/MMD-critic</a>.</p>
<p>prototype を見つけるための、最もシンプルな代替手法は以下です。 <a href="https://en.wikipedia.org/wiki/K-medoids">k-medoids</a> by Kaufman et. al (1987).<a href="#fn62" class="footnoteRef" id="fnref62"><sup>62</sup></a></p>

<!--{pagebreak}-->
<!-- Using archetypes for interpretability:  -->
<!-- - Find archetypes -->
<!-- - Get predictions for archetypes -->
<!-- - Interpret -->

<!--{pagebreak}-->
<!-- ## Influential Instances {#influential} -->
</div>
</div>
<div class="footnotes">
<hr />
<ol start="61">
<li id="fn61"><p>Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. &quot;Examples are not enough, learn to criticize! Criticism for interpretability.&quot; Advances in Neural Information Processing Systems (2016).<a href="proto.html#fnref61">↩</a></p></li>
<li id="fn62"><p>Kaufman, Leonard, and Peter Rousseeuw. &quot;Clustering by means of medoids&quot;. North-Holland (1987).<a href="proto.html#fnref62">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="adversarial.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="influential.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/06.3-example-based-proto.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
