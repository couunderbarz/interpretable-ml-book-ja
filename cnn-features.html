<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.1 学習された特徴量 | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="7.1 学習された特徴量 | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.1 学習された特徴量 | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2021-05-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="neural-networks.html"/>
<link rel="next" href="future.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>要約</a></li>
<li class="chapter" data-level="" data-path="著者による序文.html"><a href="著者による序文.html"><i class="fa fa-check"></i>著者による序文</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> イントロダクション</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> 物語の時間</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#稲妻は二度と打たない"><i class="fa fa-check"></i>稲妻は二度と打たない</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#信用失墜"><i class="fa fa-check"></i>信用失墜</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#フェルミのペーパークリップ"><i class="fa fa-check"></i>フェルミのペーパー・クリップ</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="機械学習とは何か.html"><a href="機械学習とは何か.html"><i class="fa fa-check"></i><b>1.2</b> 機械学習とは何か？</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> 専門用語</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> 解釈可能性</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> 解釈可能性の重要性</a></li>
<li class="chapter" data-level="2.2" data-path="解釈可能な手法の分類.html"><a href="解釈可能な手法の分類.html"><i class="fa fa-check"></i><b>2.2</b> 解釈可能な手法の分類</a></li>
<li class="chapter" data-level="2.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html"><i class="fa fa-check"></i><b>2.3</b> 解釈可能性の範囲</a><ul>
<li class="chapter" data-level="2.3.1" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#アルゴリズムの透明性"><i class="fa fa-check"></i><b>2.3.1</b> アルゴリズムの透明性</a></li>
<li class="chapter" data-level="2.3.2" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#全体的なモデルの解釈可能性"><i class="fa fa-check"></i><b>2.3.2</b> 全体的なモデルの解釈可能性</a></li>
<li class="chapter" data-level="2.3.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#モジュールレベルのモデルの全体的な解釈可能性"><i class="fa fa-check"></i><b>2.3.3</b> モジュールレベルのモデルの全体的な解釈可能性</a></li>
<li class="chapter" data-level="2.3.4" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#単一の予測に対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.4</b> 単一の予測に対する局所的な解釈</a></li>
<li class="chapter" data-level="2.3.5" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#予測のグループに対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.5</b> 予測のグループに対する局所的な解釈</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="解釈可能性の評価.html"><a href="解釈可能性の評価.html"><i class="fa fa-check"></i><b>2.4</b> 解釈可能性の評価</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> 説明に関する性質</a></li>
<li class="chapter" data-level="2.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.6</b> 人間に優しい説明</a><ul>
<li class="chapter" data-level="2.6.1" data-path="explanation.html"><a href="explanation.html#説明とはなにか"><i class="fa fa-check"></i><b>2.6.1</b> 説明とはなにか</a></li>
<li class="chapter" data-level="2.6.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>2.6.2</b> よい説明とは何か?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> データセット</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> 自転車レンタル (回帰)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube スパムコメント (テキスト分類)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> 子宮頸がんのリスク要因(クラス分類)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> 解釈可能なモデル</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> 線形回帰</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#解釈"><i class="fa fa-check"></i><b>4.1.1</b> 解釈</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#例"><i class="fa fa-check"></i><b>4.1.2</b> 例</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#可視化による解釈"><i class="fa fa-check"></i><b>4.1.3</b> 可視化による解釈</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#個々の予測に対する説明"><i class="fa fa-check"></i><b>4.1.4</b> 個々の予測に対する説明</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#カテゴリカル特徴量のエンコーディング"><i class="fa fa-check"></i><b>4.1.5</b> カテゴリカル特徴量のエンコーディング</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#線形モデルは良い説明を与えるか"><i class="fa fa-check"></i><b>4.1.6</b> 線形モデルは良い説明を与えるか?</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.7</b> スパースな線形モデル</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#長所"><i class="fa fa-check"></i><b>4.1.8</b> 長所</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#短所"><i class="fa fa-check"></i><b>4.1.9</b> 短所</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> ロジスティック回帰</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#線形回帰モデルを分類のために使うと何がいけないか"><i class="fa fa-check"></i><b>4.2.1</b> 線形回帰モデルを分類のために使うと何がいけないか。</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic.html"><a href="logistic.html#理論"><i class="fa fa-check"></i><b>4.2.2</b> 理論</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic.html"><a href="logistic.html#解釈性"><i class="fa fa-check"></i><b>4.2.3</b> 解釈性</a></li>
<li class="chapter" data-level="4.2.4" data-path="logistic.html"><a href="logistic.html#例-1"><i class="fa fa-check"></i><b>4.2.4</b> 例</a></li>
<li class="chapter" data-level="4.2.5" data-path="logistic.html"><a href="logistic.html#長所と短所"><i class="fa fa-check"></i><b>4.2.5</b> 長所と短所</a></li>
<li class="chapter" data-level="4.2.6" data-path="logistic.html"><a href="logistic.html#ソフトウェア"><i class="fa fa-check"></i><b>4.2.6</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>4.3</b> GLM、GAM、その他</a><ul>
<li class="chapter" data-level="4.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>4.3.1</b> 結果が正規分布に従わない場合 - GLMs</a></li>
<li class="chapter" data-level="4.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>4.3.2</b> 相互作用</a></li>
<li class="chapter" data-level="4.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>4.3.3</b> 非線形効果 - GAM</a></li>
<li class="chapter" data-level="4.3.4" data-path="extend-lm.html"><a href="extend-lm.html#長所-1"><i class="fa fa-check"></i><b>4.3.4</b> 長所</a></li>
<li class="chapter" data-level="4.3.5" data-path="extend-lm.html"><a href="extend-lm.html#短所-1"><i class="fa fa-check"></i><b>4.3.5</b> 短所</a></li>
<li class="chapter" data-level="4.3.6" data-path="extend-lm.html"><a href="extend-lm.html#ソフトウェア-1"><i class="fa fa-check"></i><b>4.3.6</b> ソフトウェア</a></li>
<li class="chapter" data-level="4.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>4.3.7</b> さらなる拡張</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.4</b> 決定木</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree.html"><a href="tree.html#決定木の解釈"><i class="fa fa-check"></i><b>4.4.1</b> 決定木の解釈</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree.html"><a href="tree.html#例-2"><i class="fa fa-check"></i><b>4.4.2</b> 例</a></li>
<li class="chapter" data-level="4.4.3" data-path="tree.html"><a href="tree.html#長所-2"><i class="fa fa-check"></i><b>4.4.3</b> 長所</a></li>
<li class="chapter" data-level="4.4.4" data-path="tree.html"><a href="tree.html#短所-2"><i class="fa fa-check"></i><b>4.4.4</b> 短所</a></li>
<li class="chapter" data-level="4.4.5" data-path="tree.html"><a href="tree.html#ソフトウェア-2"><i class="fa fa-check"></i><b>4.4.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>4.5</b> 決定規則</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rules.html"><a href="rules.html#単一の特徴量による規則学習-oner"><i class="fa fa-check"></i><b>4.5.1</b> 単一の特徴量による規則学習 (OneR)</a></li>
<li class="chapter" data-level="4.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>4.5.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="4.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>4.5.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="4.5.4" data-path="rules.html"><a href="rules.html#長所-3"><i class="fa fa-check"></i><b>4.5.4</b> 長所</a></li>
<li class="chapter" data-level="4.5.5" data-path="rules.html"><a href="rules.html#短所-3"><i class="fa fa-check"></i><b>4.5.5</b> 短所</a></li>
<li class="chapter" data-level="4.5.6" data-path="rules.html"><a href="rules.html#ソフトウェアと代替手法"><i class="fa fa-check"></i><b>4.5.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.6</b> RuleFit</a><ul>
<li class="chapter" data-level="4.6.1" data-path="rulefit.html"><a href="rulefit.html#解釈と例"><i class="fa fa-check"></i><b>4.6.1</b> 解釈と例</a></li>
<li class="chapter" data-level="4.6.2" data-path="rulefit.html"><a href="rulefit.html#理論-1"><i class="fa fa-check"></i><b>4.6.2</b> 理論</a></li>
<li class="chapter" data-level="4.6.3" data-path="rulefit.html"><a href="rulefit.html#長所-4"><i class="fa fa-check"></i><b>4.6.3</b> 長所</a></li>
<li class="chapter" data-level="4.6.4" data-path="rulefit.html"><a href="rulefit.html#短所-4"><i class="fa fa-check"></i><b>4.6.4</b> 短所</a></li>
<li class="chapter" data-level="4.6.5" data-path="rulefit.html"><a href="rulefit.html#ソフトウェアと代替手法-1"><i class="fa fa-check"></i><b>4.6.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>4.7</b> その他の解釈可能なモデル</a><ul>
<li class="chapter" data-level="4.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#単純ベイズ分類器-naive-bayes-classifier"><i class="fa fa-check"></i><b>4.7.1</b> 単純ベイズ分類器 (Naive Bayes Classifier)</a></li>
<li class="chapter" data-level="4.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k近傍法"><i class="fa fa-check"></i><b>4.7.2</b> k近傍法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> モデル非依存(Model-Agnostic)な手法</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#例-3"><i class="fa fa-check"></i><b>5.1.1</b> 例</a></li>
<li class="chapter" data-level="5.1.2" data-path="pdp.html"><a href="pdp.html#長所-5"><i class="fa fa-check"></i><b>5.1.2</b> 長所</a></li>
<li class="chapter" data-level="5.1.3" data-path="pdp.html"><a href="pdp.html#短所-5"><i class="fa fa-check"></i><b>5.1.3</b> 短所</a></li>
<li class="chapter" data-level="5.1.4" data-path="pdp.html"><a href="pdp.html#ソフトウェアと代替手法-2"><i class="fa fa-check"></i><b>5.1.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ice.html"><a href="ice.html#例-4"><i class="fa fa-check"></i><b>5.2.1</b> 例</a></li>
<li class="chapter" data-level="5.2.2" data-path="ice.html"><a href="ice.html#長所-6"><i class="fa fa-check"></i><b>5.2.2</b> 長所</a></li>
<li class="chapter" data-level="5.2.3" data-path="ice.html"><a href="ice.html#短所-6"><i class="fa fa-check"></i><b>5.2.3</b> 短所</a></li>
<li class="chapter" data-level="5.2.4" data-path="ice.html"><a href="ice.html#ソフトウェアと代替手法-3"><i class="fa fa-check"></i><b>5.2.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5.3</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ale.html"><a href="ale.html#モチベーションと直感"><i class="fa fa-check"></i><b>5.3.1</b> モチベーションと直感</a></li>
<li class="chapter" data-level="5.3.2" data-path="ale.html"><a href="ale.html#理論-2"><i class="fa fa-check"></i><b>5.3.2</b> 理論</a></li>
<li class="chapter" data-level="5.3.3" data-path="ale.html"><a href="ale.html#予測"><i class="fa fa-check"></i><b>5.3.3</b> 予測</a></li>
<li class="chapter" data-level="5.3.4" data-path="ale.html"><a href="ale.html#例-6"><i class="fa fa-check"></i><b>5.3.4</b> 例</a></li>
<li class="chapter" data-level="5.3.5" data-path="ale.html"><a href="ale.html#利点"><i class="fa fa-check"></i><b>5.3.5</b> 利点</a></li>
<li class="chapter" data-level="5.3.6" data-path="ale.html"><a href="ale.html#欠点"><i class="fa fa-check"></i><b>5.3.6</b> 欠点</a></li>
<li class="chapter" data-level="5.3.7" data-path="ale.html"><a href="ale.html#実装と代替手法"><i class="fa fa-check"></i><b>5.3.7</b> 実装と代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.4</b> 特徴量の相互作用</a><ul>
<li class="chapter" data-level="5.4.1" data-path="interaction.html"><a href="interaction.html#特徴量の相互作用とは"><i class="fa fa-check"></i><b>5.4.1</b> 特徴量の相互作用とは</a></li>
<li class="chapter" data-level="5.4.2" data-path="interaction.html"><a href="interaction.html#friedman-の-h統計量の理論"><i class="fa fa-check"></i><b>5.4.2</b> Friedman の H統計量の理論</a></li>
<li class="chapter" data-level="5.4.3" data-path="interaction.html"><a href="interaction.html#例-7"><i class="fa fa-check"></i><b>5.4.3</b> 例</a></li>
<li class="chapter" data-level="5.4.4" data-path="interaction.html"><a href="interaction.html#利点-1"><i class="fa fa-check"></i><b>5.4.4</b> 利点</a></li>
<li class="chapter" data-level="5.4.5" data-path="interaction.html"><a href="interaction.html#欠点-1"><i class="fa fa-check"></i><b>5.4.5</b> 欠点</a></li>
<li class="chapter" data-level="5.4.6" data-path="interaction.html"><a href="interaction.html#実装"><i class="fa fa-check"></i><b>5.4.6</b> 実装</a></li>
<li class="chapter" data-level="5.4.7" data-path="interaction.html"><a href="interaction.html#代替手法"><i class="fa fa-check"></i><b>5.4.7</b> 代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.5</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="5.5.1" data-path="feature-importance.html"><a href="feature-importance.html#理論-3"><i class="fa fa-check"></i><b>5.5.1</b> 理論</a></li>
<li class="chapter" data-level="5.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>5.5.2</b> 特徴量の重要度は、学習データとテストデータのどちらで計算するべきか</a></li>
<li class="chapter" data-level="5.5.3" data-path="feature-importance.html"><a href="feature-importance.html#例と解釈"><i class="fa fa-check"></i><b>5.5.3</b> 例と解釈</a></li>
<li class="chapter" data-level="5.5.4" data-path="feature-importance.html"><a href="feature-importance.html#利点-2"><i class="fa fa-check"></i><b>5.5.4</b> 利点</a></li>
<li class="chapter" data-level="5.5.5" data-path="feature-importance.html"><a href="feature-importance.html#欠点-2"><i class="fa fa-check"></i><b>5.5.5</b> 欠点</a></li>
<li class="chapter" data-level="5.5.6" data-path="feature-importance.html"><a href="feature-importance.html#ソフトウェアと代替手法-4"><i class="fa fa-check"></i><b>5.5.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.6</b> グローバルサロゲート (Global Surrogate)</a><ul>
<li class="chapter" data-level="5.6.1" data-path="global.html"><a href="global.html#理論-4"><i class="fa fa-check"></i><b>5.6.1</b> 理論</a></li>
<li class="chapter" data-level="5.6.2" data-path="global.html"><a href="global.html#例-8"><i class="fa fa-check"></i><b>5.6.2</b> 例</a></li>
<li class="chapter" data-level="5.6.3" data-path="global.html"><a href="global.html#長所-7"><i class="fa fa-check"></i><b>5.6.3</b> 長所</a></li>
<li class="chapter" data-level="5.6.4" data-path="global.html"><a href="global.html#短所-7"><i class="fa fa-check"></i><b>5.6.4</b> 短所</a></li>
<li class="chapter" data-level="5.6.5" data-path="global.html"><a href="global.html#ソフトウェア-3"><i class="fa fa-check"></i><b>5.6.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.7</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="5.7.1" data-path="lime.html"><a href="lime.html#表形式データにおける-lime"><i class="fa fa-check"></i><b>5.7.1</b> 表形式データにおける LIME</a></li>
<li class="chapter" data-level="5.7.2" data-path="lime.html"><a href="lime.html#テキストデータに対するlime"><i class="fa fa-check"></i><b>5.7.2</b> テキストデータに対するLIME</a></li>
<li class="chapter" data-level="5.7.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.7.3</b> 画像データに対するLIME</a></li>
<li class="chapter" data-level="5.7.4" data-path="lime.html"><a href="lime.html#長所-8"><i class="fa fa-check"></i><b>5.7.4</b> 長所</a></li>
<li class="chapter" data-level="5.7.5" data-path="lime.html"><a href="lime.html#短所-8"><i class="fa fa-check"></i><b>5.7.5</b> 短所</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>5.8</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="5.8.1" data-path="anchors.html"><a href="anchors.html#anchor-の発見"><i class="fa fa-check"></i><b>5.8.1</b> Anchor の発見</a></li>
<li class="chapter" data-level="5.8.2" data-path="anchors.html"><a href="anchors.html#複雑性と実行時間"><i class="fa fa-check"></i><b>5.8.2</b> 複雑性と実行時間</a></li>
<li class="chapter" data-level="5.8.3" data-path="anchors.html"><a href="anchors.html#表形式データの例"><i class="fa fa-check"></i><b>5.8.3</b> 表形式データの例</a></li>
<li class="chapter" data-level="5.8.4" data-path="anchors.html"><a href="anchors.html#長所-9"><i class="fa fa-check"></i><b>5.8.4</b> 長所</a></li>
<li class="chapter" data-level="5.8.5" data-path="anchors.html"><a href="anchors.html#短所-9"><i class="fa fa-check"></i><b>5.8.5</b> 短所</a></li>
<li class="chapter" data-level="5.8.6" data-path="anchors.html"><a href="anchors.html#ソフトウェアと代替手法-5"><i class="fa fa-check"></i><b>5.8.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.9</b> シャープレイ値 (Shapley Values)</a><ul>
<li class="chapter" data-level="5.9.1" data-path="shapley.html"><a href="shapley.html#一般的なアイデア"><i class="fa fa-check"></i><b>5.9.1</b> 一般的なアイデア</a></li>
<li class="chapter" data-level="5.9.2" data-path="shapley.html"><a href="shapley.html#例と解釈-1"><i class="fa fa-check"></i><b>5.9.2</b> 例と解釈</a></li>
<li class="chapter" data-level="5.9.3" data-path="shapley.html"><a href="shapley.html#シャープレイ値の詳細"><i class="fa fa-check"></i><b>5.9.3</b> シャープレイ値の詳細</a></li>
<li class="chapter" data-level="5.9.4" data-path="shapley.html"><a href="shapley.html#長所-10"><i class="fa fa-check"></i><b>5.9.4</b> 長所</a></li>
<li class="chapter" data-level="5.9.5" data-path="shapley.html"><a href="shapley.html#短所-10"><i class="fa fa-check"></i><b>5.9.5</b> 短所</a></li>
<li class="chapter" data-level="5.9.6" data-path="shapley.html"><a href="shapley.html#ソフトウェアと代替手法-6"><i class="fa fa-check"></i><b>5.9.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>5.10</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="5.10.1" data-path="shap.html"><a href="shap.html#定義"><i class="fa fa-check"></i><b>5.10.1</b> 定義</a></li>
<li class="chapter" data-level="5.10.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>5.10.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="5.10.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>5.10.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="5.10.4" data-path="shap.html"><a href="shap.html#例-12"><i class="fa fa-check"></i><b>5.10.4</b> 例</a></li>
<li class="chapter" data-level="5.10.5" data-path="shap.html"><a href="shap.html#shap-特徴量重要度-shap-feature-importance"><i class="fa fa-check"></i><b>5.10.5</b> SHAP 特徴量重要度 (SHAP Feature Importance)</a></li>
<li class="chapter" data-level="5.10.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>5.10.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="5.10.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>5.10.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="5.10.8" data-path="shap.html"><a href="shap.html#shap-相互作用値-shap-interaction-values"><i class="fa fa-check"></i><b>5.10.8</b> SHAP 相互作用値 (SHAP Interaction Values)</a></li>
<li class="chapter" data-level="5.10.9" data-path="shap.html"><a href="shap.html#clustering-shap-values"><i class="fa fa-check"></i><b>5.10.9</b> Clustering SHAP values</a></li>
<li class="chapter" data-level="5.10.10" data-path="shap.html"><a href="shap.html#長所-11"><i class="fa fa-check"></i><b>5.10.10</b> 長所</a></li>
<li class="chapter" data-level="5.10.11" data-path="shap.html"><a href="shap.html#短所-11"><i class="fa fa-check"></i><b>5.10.11</b> 短所</a></li>
<li class="chapter" data-level="5.10.12" data-path="shap.html"><a href="shap.html#ソフトウェア-4"><i class="fa fa-check"></i><b>5.10.12</b> ソフトウェア</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>6</b> 例示に基づいた説明手法</a><ul>
<li class="chapter" data-level="6.1" data-path="反事実的.html"><a href="反事実的.html"><i class="fa fa-check"></i><b>6.1</b> 反事実的説明 (Counterfactual Explanations)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="反事実的.html"><a href="反事実的.html#反事実的説明の生成"><i class="fa fa-check"></i><b>6.1.1</b> 反事実的説明の生成</a></li>
<li class="chapter" data-level="6.1.2" data-path="反事実的.html"><a href="反事実的.html#例-13"><i class="fa fa-check"></i><b>6.1.2</b> 例</a></li>
<li class="chapter" data-level="6.1.3" data-path="反事実的.html"><a href="反事実的.html#長所-12"><i class="fa fa-check"></i><b>6.1.3</b> 長所</a></li>
<li class="chapter" data-level="6.1.4" data-path="反事実的.html"><a href="反事実的.html#短所-12"><i class="fa fa-check"></i><b>6.1.4</b> 短所</a></li>
<li class="chapter" data-level="6.1.5" data-path="反事実的.html"><a href="反事実的.html#ソフトウェアと代替手法-7"><i class="fa fa-check"></i><b>6.1.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>6.2</b> 敵対的サンプル (Adversarial Examples)</a><ul>
<li class="chapter" data-level="6.2.1" data-path="adversarial.html"><a href="adversarial.html#手法及び例"><i class="fa fa-check"></i><b>6.2.1</b> 手法及び例</a></li>
<li class="chapter" data-level="6.2.2" data-path="adversarial.html"><a href="adversarial.html#サイバーセキュリティーの観点"><i class="fa fa-check"></i><b>6.2.2</b> サイバーセキュリティーの観点</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>6.3</b> prototype と criticism</a><ul>
<li class="chapter" data-level="6.3.1" data-path="proto.html"><a href="proto.html#理論-5"><i class="fa fa-check"></i><b>6.3.1</b> 理論</a></li>
<li class="chapter" data-level="6.3.2" data-path="proto.html"><a href="proto.html#例-14"><i class="fa fa-check"></i><b>6.3.2</b> 例</a></li>
<li class="chapter" data-level="6.3.3" data-path="proto.html"><a href="proto.html#長所-13"><i class="fa fa-check"></i><b>6.3.3</b> 長所</a></li>
<li class="chapter" data-level="6.3.4" data-path="proto.html"><a href="proto.html#短所-13"><i class="fa fa-check"></i><b>6.3.4</b> 短所</a></li>
<li class="chapter" data-level="6.3.5" data-path="proto.html"><a href="proto.html#コードと代替手法"><i class="fa fa-check"></i><b>6.3.5</b> コードと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>6.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="6.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>6.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="6.4.2" data-path="influential.html"><a href="influential.html#影響関数-influence-functions"><i class="fa fa-check"></i><b>6.4.2</b> 影響関数 (Influence Functions)</a></li>
<li class="chapter" data-level="6.4.3" data-path="influential.html"><a href="influential.html#長所-14"><i class="fa fa-check"></i><b>6.4.3</b> 長所</a></li>
<li class="chapter" data-level="6.4.4" data-path="influential.html"><a href="influential.html#短所-14"><i class="fa fa-check"></i><b>6.4.4</b> 短所</a></li>
<li class="chapter" data-level="6.4.5" data-path="influential.html"><a href="influential.html#ソフトウェアと代替手法-8"><i class="fa fa-check"></i><b>6.4.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> ニューラルネットワークの解釈</a><ul>
<li class="chapter" data-level="7.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>7.1</b> 学習された特徴量</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cnn-features.html"><a href="cnn-features.html#特徴量の可視化"><i class="fa fa-check"></i><b>7.1.1</b> 特徴量の可視化</a></li>
<li class="chapter" data-level="7.1.2" data-path="cnn-features.html"><a href="cnn-features.html#ネットワークの解剖"><i class="fa fa-check"></i><b>7.1.2</b> ネットワークの解剖</a></li>
<li class="chapter" data-level="7.1.3" data-path="cnn-features.html"><a href="cnn-features.html#利点-3"><i class="fa fa-check"></i><b>7.1.3</b> 利点</a></li>
<li class="chapter" data-level="7.1.4" data-path="cnn-features.html"><a href="cnn-features.html#欠点-3"><i class="fa fa-check"></i><b>7.1.4</b> 欠点</a></li>
<li class="chapter" data-level="7.1.5" data-path="cnn-features.html"><a href="cnn-features.html#ソフトウェアとその他の資料"><i class="fa fa-check"></i><b>7.1.5</b> ソフトウェアとその他の資料</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> 解釈可能な機械学習の未来</a><ul>
<li class="chapter" data-level="8.1" data-path="機械学習の未来.html"><a href="機械学習の未来.html"><i class="fa fa-check"></i><b>8.1</b> 機械学習の未来</a></li>
<li class="chapter" data-level="8.2" data-path="解釈性の未来.html"><a href="解釈性の未来.html"><i class="fa fa-check"></i><b>8.2</b> 解釈性の未来</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> 著者貢献</a></li>
<li class="chapter" data-level="10" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>10</b> この本の引用</a></li>
<li class="chapter" data-level="11" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>11</b> 翻訳</a></li>
<li class="chapter" data-level="12" data-path="謝辞.html"><a href="謝辞.html"><i class="fa fa-check"></i><b>12</b> 謝辞</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used-for-examples.html"><a href="r-packages-used-for-examples.html"><i class="fa fa-check"></i>R Packages Used for Examples</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="cnn-features" class="section level2">
<h2><span class="header-section-number">7.1</span> 学習された特徴量</h2>
<!--## Learned Features {#cnn-features}-->
<p><em>This chapter is currently only available in this web version. ebook and print will follow.</em></p>
<!--
Convolutional neural networks learn abstract features and concepts from raw image pixels.
[Feature Visualization](#feature-visualization) visualizes the learned features by activation maximization.
[Network Dissection](#network-dissection) labels neural network units (e.g. channels) with human concepts.
-->
<p>畳み込みニューラルネットワークは抽象的な特徴量や、元画像ピクセルから概念を学習します。 <a href="#feature-visualization">特徴量の可視化</a>では学習された特徴量を activation maximization により可視化します。 <a href="#network-dissection">ネットワーク分析</a>ではニューラルネットワークのユニット（チャネルなど）を人間の理解によってラベル付けします。</p>
<!-- Background: Why feature visualization -->
<!--Deep neural networks learn high-level features in the hidden layers.
This is one of their greatest strengths and reduces the need for feature engineering.
Assume you want to build an image classifier with a support vector machine.
The raw pixel matrices are not the best input for training your SVM, so you create new features based on color, frequency domain, edge detectors and so on.
With convolutional neural networks, the image is fed into the network in its raw form (pixels).
The network transforms the image many times.
First, the image goes through many convolutional layers.
In those convolutional layers, the network learns new and increasingly complex features in its layers.
Then the transformed image information goes through the fully connected layers and turns into a classification or prediction.
-->
<!--背景: なぜ特徴量の可視化をするのか-->
<p>深層ニューラルネットワークは隠れ層で高レベルの特徴量を学習します。 これは深層ニューラルネットワークの最も重要な強みの1つで、特徴量に関する作業を減らします。 サポートベクターマシンによる画像分類器を構成したいと仮定して下さい。 生のピクセル行列は SVM の訓練に最も良い入力形式ではないため、色、周波数領域、輪郭検出器などの新たな特徴量を作りだす必要があります。 畳み込みニューラルネットワークでは、画像は生の形式(ピクセル)のままネットワークに与えられます。 ネットワークは画像を何度も変換します。 初めに、画像は多数の畳み込み層を通過します。 それらの畳み込み層では、ネットワークは新たな、そして次第に複雑な特徴量をその層において学習します。 その後、変換された画像の情報は全結合層を通過し、分類や予測へ変化します。</p>
<!--
fig.cap = "Architecture of Inception V1 neural network. Each enumerated unit (3a to 5b) represents a layer with differently sized convolutions and pooling. Figure from Olah, et al. 2019 (CC-BY 4.0) https://distill.pub/2017/activation-atlas/."
-->
<!--
fig.cap = "Features learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from simple features in the lower convolutional layers (left) to more abstract features in the higher convolutional layers (right). Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/."
-->
<div class="figure"><span id="fig:unnamed-chunk-49"></span>
<img src="images/cnn-features.png" alt="ImageNet データにより訓練された畳み込みニューラルネットワーク (Inception V1) による、学習された特徴量。特徴量は、低い畳み込み層(左)の単純な特徴量から、高い畳み込み層(右)のより抽象的な特徴量までの範囲を持つ。Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/." width="800" />
<p class="caption">
FIGURE 7.1: ImageNet データにより訓練された畳み込みニューラルネットワーク (Inception V1) による、学習された特徴量。特徴量は、低い畳み込み層(左)の単純な特徴量から、高い畳み込み層(右)のより抽象的な特徴量までの範囲を持つ。Olah, et al. 2017 (CC-BY 4.0) <a href="https://distill.pub/2017/feature-visualization/appendix/" class="uri">https://distill.pub/2017/feature-visualization/appendix/</a>.
</p>
</div>
<!--
- The first convolutional layer(s) learn features such as edges and simple textures.
- Later convolutional layers learn features such as more complex textures and patterns.
- The last convolutional layers learn features such as objects or parts of objects.
- The fully connected layers learn to connect the activations from the high-level features to the individual classes to be predicted.
-->
<ul>
<li>最初の畳み込み層は輪郭や単純なテクスチャといった特徴量を学習します。</li>
<li>その後の畳み込み層はより複雑なテクスチャや模様といった特徴量を学習します。</li>
<li>最後の特徴量は物体や物体の一部といた特徴量を学習します。</li>
<li>全結合層は、高レベル特徴量からの活性化を予測されるべき個別のクラスへと接続するよう学習します。</li>
</ul>
<!--
Cool.
But how do we actually get those hallucinatory images?
-->
<p>クールですね。 しかし、実際にはどのようにそんな幻覚めいた画像を得ているのでしょうか？</p>
<div id="特徴量の可視化" class="section level3">
<h3><span class="header-section-number">7.1.1</span> 特徴量の可視化</h3>
<!--### Feature Visualization {#feature-visualization}-->
<!-- Feature Visualization explained-->
<!--The approach of making the learned features explicit is called **Feature Visualization**.
Feature visualization for a unit of a neural network is done by finding the input that maximizes the activation of that unit.-->
<p>学習された特徴量を明示的なものにしようとする試みを<strong>特徴量の可視化</strong>と呼びます。 ニューラルネットワークのあるユニットの特徴量の可視化はその部分の活性化関数を最大化する入力を見つけることによって行います。</p>
<!--"Unit" refers either to individual neurons, channels (also called feature maps), entire layers or the final class probability in classification (or the corresponding pre-softmax neuron, which is recommended).
Individual neurons are atomic units of the network, so we would get the most information by creating feature visualizations for each neuron.-->
<p>「ユニット」はそれぞれのニューロン、チャンネル(特徴量マップとも呼ばれる)、全体の層、およびクラス分類における最終的なクラスの確率(や対応するソフトマックスの前のニューロン、これが推奨される)のことを指します。</p>
<!--But there is a problem: 
Neural networks often contain millions of neurons.
Looking at each neuron's feature visualization would take too long.
The channels (sometimes called activation maps) as units are a good choice for feature visualization.
We can go one step further and visualize an entire convolutional layer.
Layers as a unit are used for Google's DeepDream, which repeatedly adds the visualized features of a layer to the original image, resulting in a dream-like version of the input.-->
<p>しかし、問題があります。 ニューラルネットはしばしば何百万ものニューロンを含むことがあります。 それぞれのニューロンの特徴量の可視化を見ることはあまりにも時間がかかりすぎるでしょう。 ユニットとしてのチャンネル(activation map とも呼ばれることもある) は特徴量を可視化するのに良い選択となります。 もう一段上がって全体の畳み込み層を可視化できます。 ユニットとしての層が Google の DeepDream で使われています、これは元の画像に繰り返し可視化された特徴量を加えることで、入力の夢バージョンを出力するものです。</p>
<!--
fig.cap="Feature visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron (or corresponding pre-softmax neuron)"
-->
<div class="figure"><span id="fig:units"></span>
<img src="images/units.jpg" alt="特徴量の可視化は異なるユニットで行うことができる。A)畳み込みニューロン、B)畳み込みチャンネル、C)畳み込み層、D)ニューロン、E)隠れ層、F)クラス確率を示すニューロン(または、それに対応するソフトマックスの前のニューロン)" width="800" />
<p class="caption">
FIGURE 7.2: 特徴量の可視化は異なるユニットで行うことができる。A)畳み込みニューロン、B)畳み込みチャンネル、C)畳み込み層、D)ニューロン、E)隠れ層、F)クラス確率を示すニューロン(または、それに対応するソフトマックスの前のニューロン)
</p>
</div>
<!--
fig.cap="Optimized images for Inception V1 (channels mixed3a, mixed4c, mixed4d and mixed5a). Images are maximized for a random direction of the activations. Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/."
-->
<div id="最適化を通した特徴量の可視化" class="section level4">
<h4><span class="header-section-number">7.1.1.1</span> 最適化を通した特徴量の可視化</h4>
<!--#### Feature Visualization through Optimization-->
<!--
In mathematical terms, feature visualization is an optimization problem.
We assume that the weights of the neural network are fixed, which means that the network is trained.
We are looking for a new image that maximizes the (mean) activation of a unit, here a single neuron:-->
<p>数学的な観点では、特徴量の可視化は最適化問題になります。 ニューラルネットワークの重みが固定されていると仮定します。これはネットワークが学習済みであることを意味します。 ユニット、ここでは1つのニューロンを指しますが、の活性化関数の計算結果(の平均)を最大化する新しい画像を探します。</p>
<p><span class="math display">\[img^*=\arg\max_{img}h_{n,x,y,z}(img)\]</span></p>
<!--The function $h$ is the activation of a neuron, *img* the input of the network (an image), x and y describe the spatial position of the neuron, n specifies the layer and z is the channel index.
For the mean activation of an entire channel z in layer n we maximize:-->
<p>関数 <span class="math inline">\(h\)</span> はニューロンの活性化関数、<em>img</em> はネットワークの入力(画像)、x と y はニューロンの空間的な位置を表していて、n は層を特定し、z はチャンネルのインデックスです。 層 n のチャンネル z の全体の活性化関数の計算結果の平均を最大化するには次式のようにします。</p>
<p><span class="math display">\[img^*=\arg\max_{img}\sum_{x,y}h_{n,x,y,z}(img)\]</span></p>
<!--In this formula, all neurons in channel z are equally weighted.
Alternatively, you can also maximize random directions, which means that the neurons would be multiplied by different parameters, including negative directions.
In this way, we study how the neurons interact within the channel.
Instead of maximizing the activation, you can also minimize the activation (which corresponds to maximizing the negative direction).
Interestingly, when you maximize the negative direction you get very different features for the same unit:-->
<p>この方程式では、チャンネル z のすべてのニューロンが等しく重み付けされています。 あるいは、ランダムな方向に最大化できて、これはニューロンが負の方向を含む様々なパラメータをかけられることを意味します。 このようにして、ニューロンがチャンネルにどのように作用するかを学びます。 活性化関数を最大化するかわりに、最小化もできます(これは負の方向に最大化することに相当します)。 面白いことに、負の方向に最大化すると同じユニットでも全く異なる特徴量が得られます。</p>
<!--```{r pos-neg, fig.cap="Positive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something which seems to have eyes yields a negative activation. Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb", out.width=800}
knitr::include_graphics("images/a484.png")
```
-->
<div class="figure"><span id="fig:pos-neg"></span>
<img src="images/a484.png" alt="Inception V1 の、ReLU の前の mixed4d 層のニューロン484 の正(左)と負(右)の活性化関数の値。ニューロンは車輪で最大に活性化されている一方、目を持つようなものが負の活性化を産んでいます。コード: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb" width="800" />
<p class="caption">
FIGURE 7.3: Inception V1 の、ReLU の前の mixed4d 層のニューロン484 の正(左)と負(右)の活性化関数の値。ニューロンは車輪で最大に活性化されている一方、目を持つようなものが負の活性化を産んでいます。コード: <a href="https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb" class="uri">https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb</a>
</p>
</div>
<!-- Why not use training data? -->
<!--We can address this optimization problem in different ways.
First, why should we generate new images?
We could simply search through our training images and select those that maximize the activation.-->
<!-- 学習データを使用してみませんか？ -->
<p>この最適化問題に異なる方法で取り組むことができます。 はじめに、なぜ新しい画像を作るべきなのでしょうか。 単純に学習する画像を検索して、活性化関数を最大化するもの選ぶこともできます。</p>
<!--This is a valid approach, but using training data has the problem that elements on the images can be correlated and we can't see what the neural network is really looking for.
If images that yield a high activation of a certain channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both.-->
<p>これは有効なアプローチですが、学習データを使うことは画像の要素が一致してしまい、ニューラルネットが本当に探しているものがわからなくなるという問題があります。 もしある特定のチャンネルをよく活性化させる画像が犬やテニスボールを示していたら、ニューラルネットワークが犬を見ているのか、テニスボールを見ているのか、はたまた両方を見ているのかわかりません。</p>
<!-- Direct optimization -->
<!--The other approach is to generate new images, starting from random noise.
To obtain meaningful visualizations, there are usually constraints on the image, e.g. that only small changes are allowed.
To reduce noise in the feature visualization, you can apply jittering, rotation or scaling to the image before the optimization step.
Other regularization options include frequency penalization (e.g. reduce variance of neighboring pixels) or generating images with learned priors, e.g. with generative adversarial networks (GANs) [^synthesize] or denoising autoencoders [^plugandplay].-->
<!-- 直接最適化 -->
<p>別のアプローチはランダムなノイズから始めて新しい画像を生成することです。 意味のある可視化を得るために、例えば少しの変化しか許容しないなど、たいてい画像に対する制約があります。 特徴量の可視化のノイズを減らすために、最適化のステップの前に摂動、回転やスケーリングを画像に適用することもあります。 他の正則化の選択肢には、周波数が高いところのペナルティ化 (近い画素の分散を減らす など)、 または、 学習されたプライアーから画像を生成する手法 (generative adversarial network(GAN) <a href="#fn66" class="footnoteRef" id="fnref66"><sup>66</sup></a>, denoising autoencoder <a href="#fn67" class="footnoteRef" id="fnref67"><sup>67</sup></a> など) があります。</p>
<!--```{r activation-optim, fig.cap="Iterative optimization from random image to maximizing activation. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/activation-optim.png")
```
-->
<div class="figure"><span id="fig:activation-optim"></span>
<img src="images/activation-optim.png" alt="ランダムな画像による活性化関数を最大化する反復的な最適化。Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/." width="800" />
<p class="caption">
FIGURE 7.4: ランダムな画像による活性化関数を最大化する反復的な最適化。Olah, et al. 2017 (CC-BY 4.0) <a href="https://distill.pub/2017/feature-visualization/" class="uri">https://distill.pub/2017/feature-visualization/</a>.
</p>
</div>
<!--
If you want to dive a lot deeper into feature visualization, take a look at the distill.pub online journal, especially the feature visualization post by Olah et al. [^distill-fv], from which I used many of the images, and also about the building blocks of interpretability [^distill-blocks].-->
<p>もし特徴量の可視化にもっと深く潜りたいなら、オンラインの論文誌 distill.pub、特に、私が画像を多く引用した Olah らによる特徴量可視化の投稿 <a href="#fn68" class="footnoteRef" id="fnref68"><sup>68</sup></a> や説明可能なブロックの構成に関するもの <a href="#fn69" class="footnoteRef" id="fnref69"><sup>69</sup></a> を見てみてください。</p>
</div>
<div id="敵対的サンプルとの繋がり" class="section level4">
<h4><span class="header-section-number">7.1.1.2</span> 敵対的サンプルとの繋がり</h4>
<!--#### Connection to Adversarial Examples-->
<!--
There is a connection between feature visualization and [adversarial examples](#adversarial):
Both techniques maximize the activation of a neural network unit.
For adversarial examples, we look for the maximum activation of the neuron for the adversarial (= incorrect) class.
One difference is the image we start with:
For adversarial examples, it is the image for which we want to generate the adversarial image.
For feature visualization it is, depending on the approach, random noise.-->
<p>特徴量の可視化と<a href="adversarial.html#adversarial">敵対的サンプル</a>には繋がりがあります。 両者の技術はニューラルネットワークのユニットの活性化を最大化します。 敵対的サンプルでは、敵対的な(間違った)クラスへのニューロンの最大活性化を探しました。 1つの違いは、スタート時の画像です: 敵対的サンプルでは、敵対的な画像を生成しようとした画像です。 特徴量の可視化では、アプローチによっては、ランダムノイズです。</p>
</div>
<div id="テキストおよび表形式データ" class="section level4">
<h4><span class="header-section-number">7.1.1.3</span> テキストおよび表形式データ</h4>
<!--#### Text and Tabular Data-->
<!--The literature focuses on feature visualization for convolutional neural networks for image recognition.
Technically, there is nothing to stop you from finding the input that maximally activates a neuron of a fully connected neural network for tabular data or a recurrent neural network for text data.
You might not call it feature visualization any longer, since the "feature" would be a tabular data input or text.
For credit default prediction, the inputs might be the number of prior credits, number of mobile contracts, address and dozens of other features.
The learned feature of a neuron would then be a certain combination of the dozens of features.
For recurrent neural networks, it is a bit nicer to visualize what the network learned:
Karpathy et. al (2015)[^viz-rnn] showed that recurrent neural networks indeed have neurons that learn interpretable features.
They trained a character-level model, which predicts the next character in the sequence from the previous characters.
Once an opening brace "(" occurred, one of the neurons got highly activated, and got de-activated when the matching closing bracket ")" occurred.
Other neurons fired at the end of a line.
Some neurons fired in URLs.
The difference to the feature visualization for CNNs is that the examples were not found through optimization, but by studying neuron activations in the training data.-->
<p>ここでは、画像認識のための畳み込みニューラルネットワークの特徴量の可視化に焦点を当てます。技術的には、表形式データの為の全結合型ニューラルネットワークやテキストデータの為の再帰型ニューラルネットワークのニューロンを最大限活性化する入力を探すのにも何の支障もありません。 債務不履行予測では、入力は事前の信用の数値、携帯電話の契約数、住所やそのほか数十もの特徴量になるかもしれません。 ニューロンの学習された特徴量はその場合、数十の特徴量のいくつかの組み合わせになるでしょう。 再帰型ニューラルネットワークでは、ネットワークが学習したものは少しだけマシなものになるでしょう。 Karpathy et. al (2015)<a href="#fn70" class="footnoteRef" id="fnref70"><sup>70</sup></a> は再帰型ニューラルネットワークが実際にニューロンが説明可能な特徴量を学習することを示しました。 彼らは文字列において前の文字から次の文字を予測するという、文字レベルのモデルを訓練しました。 括弧開き &quot;(&quot;&quot; が現れると、ニューロンの1つが強く活性化され、対応する括弧閉じ &quot;)&quot;が現れたときに不活性化しました。 他のニューロンは行の終端で活性化しました。 URLで活性化するニューロンもありました。 CNNの特徴量の可視化との違いは、そのような例が最適化を通じてではなく、学習データへのニューロン活性化を調査することで見つかったことです。</p>
<!--Some of the images seem to show well-known concepts like dog snouts or buildings.
But how can we be sure?
The Network Dissection method links human concepts with individual neural network units.
Spoiler alert: Network Dissection requires extra datasets that someone has labeled with human concepts.-->
<p>いくつかの画像は、犬の鼻や建物といった周知の概念を表すように見えます。 しかし、どのように確信できるのでしょうか。 ネットワーク解剖の手法は人間の理解とニューラルネットワークの個別のユニットを結びつけます。 ネタバレ注意: ネットワーク解剖では、誰かが人間の理解によってラベル付けした追加のデータセットが必要となります。</p>
</div>
</div>
<div id="ネットワークの解剖" class="section level3">
<h3><span class="header-section-number">7.1.2</span> ネットワークの解剖</h3>
<!--### Network Dissection {#network-dissection}-->
<!--The Network Dissection approach by Bau & Zhou et al. (2017) [^dissect] quantifies the interpretability of a unit of a convolutional neural network.
It links highly activated areas of CNN channels with human concepts (objects, parts, textures, colors, ...).
-->
<p>Bau と Zhou ら(2017)<a href="#fn71" class="footnoteRef" id="fnref71"><sup>71</sup></a> による「ネットワークの解剖」の手法によって畳み込みニューラルネットワークのユニットの解釈性が定量化されました。 CNN チャンネルの大きく活性化している領域を人間の概念(物体、経路、模様、色...)で結びつけました。</p>
<!--The channels of a convolutional neural network learn new features, as we saw in the chapter on [Feature Visualization](#feature-visualization).
But these visualizations do not prove that a unit has learned a certain concept.
We also do not have a measure for how well a unit detects e.g. sky scrapers.
Before we go into the details of Network Dissection, we have to talk about the big hypothesis that is behind that line of research.
The hypothesis is:
Units of a neural network (like convolutional channels) learn disentangled concepts.-->
<p><a href="#feature-visualization">特徴量の可視化</a> の章で見たように、畳み込みニューラルネットワークのチャンネルは新たな特徴量を学習します。 しかしこの可視化はあるユニットがある特定の概念を学習したということではありません。 あるユニットが例えば摩天楼をどれくらいよく検出しているかを測る方法もありません。 ネットワークの解剖の詳細に向かう前に、その研究の主題の前にある大きな仮説について話さなければなりません。 その仮説とは: (畳み込みチャンネルのような)ニューラルネットワークのあるユニットは解きほぐされた概念を学習する事です。</p>
<p><strong>解きほぐされた特徴量の問題</strong> <!--**The Question of Disentangled Features**--></p>
<!--Do (convolutional) neural networks learn disentangled features?
Disentangled features mean that individual network units detect specific real world concepts.
Convolutional channel 394 might detect sky scrapers, channel 121 dog snouts, channel 12 stripes at 30 degree angle ...
The opposite of a disentangled network is a completely entangled network.
In a completely entangled network, for example, there would be no individual unit for dog snouts.
All channels would contribute to the recognition of dog snouts.-->
<p>(畳み込み)ニューラルネットワークは解きほぐされた特徴量を学習するのでしょうか？ 解きほぐされた特徴量とはネットワークの個々のユニットが現実世界の特定の概念を 検出することを意味します。 畳み込みチャンネル394は摩天楼を検出し、チャンネル121はイヌの鼻を検出し、チャンネル12は30度の角度で縞をつける...などです。 解きほぐされたネットワークの反対は完全にもつれたネットワークです。 完全にもつれたネットワークでは、例えば、イヌの鼻を検出する個々のユニットはないでしょう。 すべてのチャンネルがイヌの鼻の認識に役立っています。</p>
<!--Disentangled features imply that the network is highly interpretable.
Let us assume we have a network with completely disentangled units that are labeled  with known concepts.
This would open up the possibility to track the network's decision making process.
For example, we could analyze how the network classifies wolves against huskeys.
First, we identify the "huskey"-unit.
We can check whether this unit depends on the "dog snout", "fluffy fur" and "snow"-units from the previous layer.
If it does, we know that it will misclassify an image of a huskey with a snowy background as a wolf.
In a disentangled network, we could identify problematic non-causal correlations.
We could automatically list all highly activated units and their concepts to explain an individual prediction.
We could easily detect bias in the neural network.
For example, did the network learn a "white skin" feature to predict salary?

Spoiler-alert: Convolutional neural networks are not perfectly disentangled.
We will now look more closely at Network Dissection to find out how interpretable neural networks are.-->
<p>特徴量が解きほぐされていることはネットワークがよく説明できることを示しています。 知っている概念でラベル付けされた完全に解きほぐされたユニットをもつネットワークを感得てみましょう。 これによってネットワークの決定がなされる過程を追跡できる可能性を広がります。 例えば、ネットワークがどのようにオオカミとハスキーを分離しているかを解析できます。 はじめに、「ハスキーの」ユニットを特定します。 そしてそのユニットが前の層から「イヌの鼻」「ふわふわな毛皮」「雪」のユニットのうちどれに依存しているかを確認します。 そうすれば、そのネットワークが雪の背景のハスキーの画像をオオカミと間違って分類してしまうことがわかるでしょう。 解きほぐされたネットワークでは、問題がある何気なくない相関を特定できます。 それぞれの予測を説明するためによく活性化しているユニットとそれが何の概念についてかをすべて自動的にリストアップできます。 そのニューラルネットワークの傾向を簡単に探すことができます。 例えば、給料を予測するためにネットワークが「白い肌」の特徴量を学習するでしょうか。</p>
<p>ネタバレ注意: 畳み込みニューラルネットワークは完全に解きほぐされているわけではありません。説明可能なニューラルネットワークがどんなものであるか見るためにネットワークの解剖についてより詳しく見ていきましょう。</p>
<div id="ネットワークの解剖のアルゴリズム" class="section level4">
<h4><span class="header-section-number">7.1.2.1</span> ネットワークの解剖のアルゴリズム</h4>
<!--#### Network Dissection Algorithm-->
<!--Network Dissection has three steps: 

1. Get images with human-labeled visual concepts, from stripes to skyscrapers.
1. Measure the CNN channel activations for these images.
1. Quantify the alignment of activations and labeled concepts.-->
<p>ネットワークの解剖は3つのステップからなっています。</p>
<ol style="list-style-type: decimal">
<li>縞から摩天楼まで人間がラベル付けした視覚的な概念を持つ画像を入手します</li>
<li>それらの画像に対する CNN チャンネルの活性化度合いを計測します</li>
<li>活性化度合いとラベル付けされた概念の組み合わせを定量化します</li>
</ol>
<!--The following figure visualizes how an image is forwarded to a channel and matched with the labeled concepts.
-->
<p>次の画像はある画像がどうチャンネルに進んでいってラベル付けされた概念と一致するかを可視化したものです。</p>
<!--
fig.cap = "For a given input image and a trained network (fixed weights), we forward propagate the image up to the target layer, upscale the activations to match the original image size and compare the maximum activations with the ground truth pixel-wise segmentation. Figure originally from Bau & Zhou et. al (2017)."
-->
<div class="figure"><span id="fig:unnamed-chunk-50"></span>
<img src="images/dissection-network.png" alt="与えられた画像と学習済みのネットワーク(重みが固定化されている)に対して、画像をターゲット層まで順伝搬して、活性化層の出力を元の画像サイズに合うようにスケールアップさせ、ground truth の pixel-wise セグメンテーションと活性化関数の最大値を比較しました。図は Bau と Zhou ら(2017)のものの引用です。" width="800" />
<p class="caption">
FIGURE 7.5: 与えられた画像と学習済みのネットワーク(重みが固定化されている)に対して、画像をターゲット層まで順伝搬して、活性化層の出力を元の画像サイズに合うようにスケールアップさせ、ground truth の pixel-wise セグメンテーションと活性化関数の最大値を比較しました。図は Bau と Zhou ら(2017)のものの引用です。
</p>
</div>
<p><strong>ステップ 1: Broden データセット</strong> <!-- **Step 1: Broden dataset** --></p>
<!--
The first difficult but crucial step is data collection.
Network Dissection requires pixel-wise labeled images with concepts of different abstraction levels (from colors to street scenes).
Bau & Zhou et. al combined a couple of datasets with pixel-wise concepts.
They called this new dataset 'Broden', which stands for broadly and densely labeled data.
The Broden dataset is segmented to the pixel level mostly, for some datasets the whole image is labeled.
Broden contains 60,000 images with over 1,000 visual concepts in different abstraction levels: 468 scenes, 585 objects, 234 parts, 32 materials, 47 textures and 11 colors.
The following figure shows sample images from the Broden dataset.
-->
<p>最初の困難だが重大なステップは、データ収集です。 ネットワーク解剖は、(色から街路の光景までのような)異なる抽象化レベルの概念によってピクセル単位でラベル付けされた画像を必要とします。 Bau &amp; Zhou らは、いくつかのデータセットをピクセル単位の概念と結びつけました。 彼らはこの新たなデータセットを、広範(broadly)かつ稠密(densely)にラベル付けされたデータであることを表して ‘Broden’ と呼びました。 Broden データセットは殆どピクセルレベルまで断片化されており、一部のデータセットでは画像全体でラベル付けされています。 Broden は 60,000 枚の画像と、異なる抽象化レベルにおける 1,000 以上の視覚概念を含んでいます: 468の場面、585の物体、234の部品、32の物質、47の質感、11の色です。 次の画像では Broden データセットの幾つかのサンプル画像を示しています。</p>
<!--
fig.cap = "Example images from the Broden dataset. Figure originally from Bau & Zhou et. al (2017)."
-->
<div class="figure"><span id="fig:unnamed-chunk-51"></span>
<img src="images/broden.png" alt="Broden データセットからの画像例。オリジナルは Bau &amp; Zhou et. al (2017) によるもの。" width="800" />
<p class="caption">
FIGURE 7.6: Broden データセットからの画像例。オリジナルは Bau &amp; Zhou et. al (2017) によるもの。
</p>
</div>
<p><strong>ステップ 2: ネットワークの活性化を読み出す</strong> <!--**Step 2: Retrieve network activations**--></p>
<!--Next, we create the masks of the top activated areas per channel and per image.
At this point the concept labels are not yet involved.-->
<p>次に、チャンネルごとおよび画像ごとに最も活性化している領域のマスクを作成します。 この時点では、概念ラベルはまだ関係ありません。</p>
<!--
- For each convolutional channel k:
    - For each image x in the Broden dataset
        - Forward propagate image x to the target layer containing channel k.
        - Extract the pixel activations of convolutional channel k: $A_k(x)$
    - Calculate distribution of pixel activations $\alpha_k$ over all images
    - Determine the 0.005-quantile level $T_k$ of activations $\alpha_k$. This means 0.5% of all activations of channel k for image x are greater than $T_k$.
    - For each image x in the Broden dataset:
        - Scale the (possibly) lower-resolution activation map $A_k(x)$ to the resolution of image x. We call the result $S_k(x)$.
        - Binarize the activation map: A pixel is either on or off, depending on whether it exceeds the activation threshold $T_k$. The new mask is $M_k(x)=S_k(x)\geq{}T_k(x)$.
-->
<ul>
<li>それぞれの畳み込みチャンネル k について:
<ul>
<li>Broden データセット内のそれぞれの画像 x について
<ul>
<li>チャンネル k を含む目的の層への、画像 x の順伝播</li>
<li>畳み込みチャンネル k のピクセル活性化の抽出: <span class="math inline">\(A_k(x)\)</span></li>
</ul></li>
<li>画像全体にわたるピクセル活性化 <span class="math inline">\(\alpha_k\)</span> の分布計算</li>
<li>活性化 <span class="math inline">\(\alpha_k\)</span> の0.005-分位点 <span class="math inline">\(T_k\)</span> の決定。これは画像 x に対するチャンネル k の全活性化のうち 0.5% は <span class="math inline">\(T_k\)</span> よりも大きいことを意味します。</li>
<li>Broden データセット内のそれぞれの画像 x について:
<ul>
<li>(場合によっては)低解像度の activation map <span class="math inline">\(A_k(x)\)</span> を画像 x の解像度にまで拡大します。この結果を <span class="math inline">\(S_k(x)\)</span> と呼びます。</li>
<li>その activation map を二値化: 活性化閾値 <span class="math inline">\(T_k\)</span> を超えるか否かにより、ピクセルがオンかオフになります。この新しいマスクを <span class="math inline">\(M_k(x)=S_k(x)\geq{}T_k(x)\)</span> とします。</li>
</ul></li>
</ul></li>
</ul>
<p><strong>ステップ 3: 活性化概念の団結</strong> <!--**Step 3: Activation-concept alignment**--> <!--
After step 2 we have one activation mask per channel and image.
These activation masks mark highly activated areas.
For each channel we want to find the human concept that activates that channel.
We find the concept by comparing the activation masks with all labeled concepts.
We quantify the alignment between activation mask k and concept mask c with the Intersection over Union (IoU) score:
--> ステップ 2により、チャンネルおよび画像ごとに1つの活性化マスクを得ました。 これらの活性化マスクは強く活性化された領域を印付けます。 各チャンネルについて、チャンネルを活性化する人間的な概念を見つけたいです。 活性化マスクと全てのラベル付けされたコンセプトを比較することで、概念を発見します。 活性化マスク k と概念マスク c の間の団結を Intersection over Union (IoU) スコアにより定量化します。</p>
<!--
$$IoU_{k,c}=\frac{\sum|M_k(x)\bigcap{}L_c(x)|}{\sum|M_k(x)\bigcup{}L_c(x)|}$$
-->
<p><span class="math display">\[IoU_{k,c}=\frac{\sum|M_k(x)\bigcap{}L_c(x)|}{\sum|M_k(x)\bigcup{}L_c(x)|}\]</span></p>
<!--
where $|\cdot|$ is the cardinality of a set.
Intersection over union compares the alignment between two areas.
$IoU_{k,c}$ can be interpreted as the accuracy with which unit k detects concept c.
We call unit k a detector of concept c when $IoU_{k,c}>0.04$. 
This threshold was chosen by Bau & Zhou et. al.
-->
<p>ここで <span class="math inline">\(|\cdot|\)</span> は集合の濃度です。 IoU は2つの領域の団結を比較します。 <span class="math inline">\(IoU_{k,c}\)</span> は、ユニット k が概念 c を検出する精度として解釈できます。 <span class="math inline">\(IoU_{k,c}&gt;0.04\)</span> のときにユニット k を概念 c の検出器と呼ぶことにします。</p>
<!--
The following figure illustrates intersection and union of activation mask and concept mask for a single image:
-->
<p>次の図は1枚の画像に対する活性化マスクと概念マスクの IoU を説明しています。</p>
<!--
fig.cap = "The Intersection over Union (IoU) is computed by comparing the human ground truth annotation and the top activated pixels."
-->
<div class="figure"><span id="fig:unnamed-chunk-52"></span>
<img src="images/dissection-dog-exemplary.jpg" alt="人間がつけた正解アノテーションと最上位活性化ピクセルを比較して計算された Intersection over Union (IoU)。" width="800" />
<p class="caption">
FIGURE 7.7: 人間がつけた正解アノテーションと最上位活性化ピクセルを比較して計算された Intersection over Union (IoU)。
</p>
</div>
<!--
The following figure shows a unit that detects dogs:
-->
<p>次の図は犬を検出するユニットを見せています。</p>
<!--
fig.cap = "Activation mask for inception_4e channel 750 which detects dogs with $IoU=0.203$. Figure originally from Bau & Zhou et. al (2017)."
-->
<div class="figure"><span id="fig:unnamed-chunk-53"></span>
<img src="images/dissection-dogs.jpeg" alt="$IoU=0.203$ で犬を検出する inception_4e のチャンネル 750 の活性化マスク。オリジナルは Bau &amp; Zhou et. al (2017) によるもの。" width="800" />
<p class="caption">
FIGURE 7.8: <span class="math inline">\(IoU=0.203\)</span> で犬を検出する inception_4e のチャンネル 750 の活性化マスク。オリジナルは Bau &amp; Zhou et. al (2017) によるもの。
</p>
</div>
</div>
<div id="実験" class="section level4">
<h4><span class="header-section-number">7.1.2.2</span> 実験</h4>
<!--#### Experiments-->
<!--
The Network Dissection authors trained different network architectures (AlexNet, VGG, GoogleNet, ResNet) from scratch on different datasets (ImageNet, Places205, Places365).
ImageNet contains 1.6 million images from 1000 classes that focus on objects.
Places205 and Places365 contain 2.4 million / 1.6 million images from 205 / 365 different scenes.
They additionally trained AlexNet on self-supervised training tasks such as predicting video frame order or colorizing images.
For many of these different settings, they counted the number of unique concept detectors as a measure of interpretability.
Here are some of the findings:
-->
<p>ネットワーク解析者達は様々なネットワーク構造 (AlexNet, VGG, GoogleNet, ResNet)を異なるデータセット(ImageNet, Places205, Places365)を用いて訓練しました。 ImageNet は物体に焦点をおいた1000のクラスの160万枚の画像を含んでいます。 Places205とPlaces365は205/365の異なるシーンからの画像240/160万枚の画像を含んでいいます。 彼らは追加で AlexNet をビデオフレームの順序や画像の色付けなどの自己管理型のタスクについて訓練しました。 これらの多くの異なる設定において、彼らは、解釈性の尺度として、unique concept detectors の数を数えました。 下記の物がいくつかの発見です。</p>
<!--
- The networks detect lower-level concepts (colors, textures) at lower layers and higher-level concepts (parts, objects) at higher layers.
  We have already seen this in the [Feature Visualizations](#feature-visualization).
- Batch normalization reduces the number of unique concept detectors.
- Many units detect the same concept.
For example, there are 95 (!) dog channels in VGG trained on ImageNet when using $IoU \geq 0.04$ as detection cutoff (4 in conv4_3, 91 in conv5_3, see [project website](http://netdissect.csail.mit.edu/dissect/vgg16_imagenet/)).
- Increasing the number of channels in a layer increases the number of interpretable units.
- Random initializations (training with different random seeds) result in slightly different numbers of interpretable units.
- ResNet is the network architecture with the largest number of unique detectors, followed by VGG, GoogleNet and AlexNet last.
- The largest number of  unique concept detectors are learned for Places356, followed by Places205 and ImageNet last.
- The number of unique concept detectors increases with the number of training iterations.-->
<ul>
<li>ネットワークは下の層で下位レベルの概念（色、テクスチャー）を検知し、上層で上位の概念を検知します（パーツ、物体）。 私たちは既にこれについて特徴量視覚化で学んでいます(#feature-visualization)。</li>
<li>バッチ正規化により、unique concept detectors の数を減らします。</li>
<li>多くのユニットは同じ概念を検知します。 例えば、 <span class="math inline">\(IoU \geq 0.04\)</span> を検知のカットオフとして使用した場合、ImageNet で訓練された VGG には 95（！）の犬のチャンネルがあります。</li>
<li>層の中のチャンネルの数を増やせば、解釈可能なユニットの数も増えます。</li>
<li>ランダムな初期化の使用によって（異なるランダムシードと共に訓練すること）少しだけ異なる数の解釈可能なユニットに行き着くことがあります。</li>
<li>Places356 で最も多くの unique concept detectors が学ばれ、次に Places205 と ImageNet が続きます。</li>
</ul>
<!--
fig.cap = "ResNet trained on Places365 has the highest number of unique detectors. AlexNet with random weights has the lowest number of unique detectors and serves as baseline. Figure originally from Bau & Zhou et. al (2017)."
-->
<div class="figure"><span id="fig:unnamed-chunk-54"></span>
<img src="images/arch-compare.png" alt="Places365で訓練されたResNetga一番多くのunique detectorsを持ちます。ランダムな重みを持つAlexNetが最も少ないunique detectorsの数を持ちベースラインの役割を果たします。. 画像は Bau &amp; Zhou et. al (2017)からの出典です。" width="540" />
<p class="caption">
FIGURE 7.9: Places365で訓練されたResNetga一番多くのunique detectorsを持ちます。ランダムな重みを持つAlexNetが最も少ないunique detectorsの数を持ちベースラインの役割を果たします。. 画像は Bau &amp; Zhou et. al (2017)からの出典です。
</p>
</div>
<!--
- Networks trained on self-supervised tasks have fewer unique detectors compared to networks trained on supervised tasks.
- In transfer learning, the concept of a channel can change. For example, a dog detector became a waterfall detector. This happened in a model that was initially trained to classify objects and then fine-tuned to classify scenes.
- In one of the experiments, the authors projected the channels onto a new rotated basis.
This was done for the VGG network trained on ImageNet.
"Rotated" does not mean that the image was rotated.
"Rotated" means that we take the 256 channels from the conv5 layer and compute new 256 channels as linear combinations of the original channels.
In the process, the channels get entangled.
Rotation reduces interpretability, i.e., the number of channels aligned with a concept decreases.
The rotation was designed to keep the performance of the model the same.
The first conclusion:
Interpretability of CNNs is axis-dependent.
This means that random combinations of channels are less likely to detect unique concepts.
The second conclusion:
 Interpretability is independent of discriminative power.
 The channels can be transformed with orthogonal transformations while the discriminative power remains the same, but interpretability decreases. 
 -->
<ul>
<li>自己管理タスクで訓練されたネットワークは教師ありタスクで訓練されたネットワークと比べて少ない unique detectors を持ちます。</li>
<li>転移学習において、チャンネルの概念は変わります。例えば、犬の検知器は滝の検知器になりました。これは、最初、物体を分類するためのモデルを fine-tuning を使い、シーンを分類するためのモデルに変更しようとした時に起こりました。</li>
<li>実験のひとつで、著者は、チャンネルを新しいローテーション基底に投影しました。 これは ImageNet で訓練された VGG ネットワークで行われました。 &quot;ローテーション&quot;は画像が回転することを意味しているのではありません。 &quot;ローテーション&quot;は、私たちは画像の conv5層から 256 のチャンネルを取得し、元のチャンネルの線形結合として新たな 256 のチャンネルを計算することを意味します。 この過程で、チャンネルはかみ合います。 ローテーションは解釈性を減少させます。ex.概念に沿った、チャンネルの数が減少します。 ローテーションはモデルの性能を同じに保つために設計されました。 最初の結論は、「CNN の解釈性は軸に依存する」です。 これは、ランダムな組み合わせのチャンネルが unique concepts を検知する確率が低いことを意味します。 2つ目の結論は、「解釈性は識別力とは無関係」です。 チャンネルは直交変換で識別力を保ったまま変換できますが、解釈性は減少します。</li>
</ul>
<!--
fig.cap = "The number of unique concept detectors decreases when the 256 channels of AlexNet conv5 (trained on ImageNet) are gradually changed to a basis using a random orthogonal transformation. Figure originally from Bau & Zhou et. al (2017)."
-->
<div class="figure"><span id="fig:unnamed-chunk-55"></span>
<img src="images/rotation-dissect.png" alt="AlexNet,conv5（ImageNetで学習済み）の256のチャンネルがランダムな直交変換を使用して徐々に変更されると、unique concept detectorsの数は減少します 画像は Bau &amp; Zhou et. al (2017).からの出典です。" width="425" />
<p class="caption">
FIGURE 7.10: AlexNet,conv5（ImageNetで学習済み）の256のチャンネルがランダムな直交変換を使用して徐々に変更されると、unique concept detectorsの数は減少します 画像は Bau &amp; Zhou et. al (2017).からの出典です。
</p>
</div>
<!--
The authors also used Network Dissection for Generative Adversarial Networks (GANs).
You can find Network Dissection for GANs on [the project's website](https://gandissect.csail.mit.edu/).
-->
<p>著者は GAN (Generative Adversarial Networks) に対してもネットワーク解剖を行なっています。 GAN に対するネットワーク解剖は、こちらの[webサイト]((<a href="https://gandissect.csail.mit.edu/)をご覧ください" class="uri">https://gandissect.csail.mit.edu/)をご覧ください</a>。</p>
</div>
</div>
<div id="利点-3" class="section level3">
<h3><span class="header-section-number">7.1.3</span> 利点</h3>
<!--### Advantages-->
<!--
Feature visualizations give **unique insight into the working of neural networks**, especially for image recognition.
Given the complexity and opacity of neural networks, feature visualization is an important step in analyzing and describing neural networks.
Through feature visualization, we have learned that neural networks first learn simple edge and texture detectors and more abstract part and object detectors in higher layers.
Network dissection expands those insights and makes interpretability of network units measurable.

Network dissection allows us to **automatically link units to concepts**, which is very convenient.

Feature visualization is a great tool to **communicate in a non-technical way how neural networks work**.

With network dissection, we can also **detect concepts beyond the classes in the classification task**.
But we need datasets that contain images with pixel-wise labeled concepts.

Feature visualization can be **combined with feature attribution methods**, which explain which pixels were important for the classification.
The combination of both methods allows to explain an individual classification along with local visualization of the learned features that were involved in the classification.
See [The Building Blocks of Interpretability from distill.pub](https://distill.pub/2018/building-blocks/).

Finally, feature visualizations make **great desktop wallpapers and T-Shirts prints**.
-->
<p>特徴量視覚化は、特に画像認識において、<strong>ニューラルネットワークの働きに対してユニークな視点</strong>を与えます。 ニューラルネットワークの複雑性と不透明性において、特徴量の視覚化はニューラルネットワークを分析、説明するための重要な一歩です。 特徴量視覚化を通して、ニューラルネットワークは最初にシンプルなエッジやテクスチャを検知し、次にもっと抽象的な部分、そして最後に物体検知の層になっていることがわかりました。 ネットワーク分析はこれらの洞察を拡大し、ネットワークユニットの解釈性を測定可能なものにします。 ネットワーク分析は、私たちに「概念を自動的にユニットとリンクさせること」を許します。これはとても便利です。 特徴量視覚化は、<strong>ニューラルネットワークがどのように動いているかを非技術的な方法で伝えるための素晴らしい手法です。 ネットワーク分析と共に、</strong>分類のタスクにおいてクラスを超えた概念**を検知できます。 しかし、私たちはピクセル単位でラベル付けされた概念を持つ画像のデータセットが必要です。</p>
</div>
<div id="欠点-3" class="section level3">
<h3><span class="header-section-number">7.1.4</span> 欠点</h3>
<!--### Disadvantages-->
<!--
**Many feature visualization images are not interpretable** at all, but contain some abstract features for which we have no words or mental concept.
  The display of feature visualizations along with training data can help.
 The images still might not reveal what the neural network reacted to and only show something like "maybe there has to be something yellow in the images".
Even with Network Dissection some channels are not linked to a human concept.
For example, layer conv5_3 from VGG trained on ImageNet has 193 channels (out of 512) that could not be matched with a human concept.
-->
<p><strong>多くの特徴量可視化画像は全く解釈可能ではありません</strong>が、それを表す言葉や精神的概念が存在しないような、いくつかの抽象的な特徴量を含んでいます。 学習データと同時に特徴量可視化を表示することは、助けになります。 画像はニューラルネットワークが何に反応し、&quot;画像に何か黄色い部分があるはずだ&quot;といったことだけを示すのかまでは、明らかにしないかもしれません。 ネットワーク解剖によっても、人間的な概念と結び付けられないチャンネルがあります。 例えば、ImageNet で訓練された VGG の conv5_3 層は(512のうち)193のチャンネルは人間的な概念と組み合わせられませんでした。</p>
<!--
There are **too many units to look at**, even when "only" visualizing the channel activations.
  For Inception V1 there are already over 5000 channels from 9 convolutional layers.
If you also want to show the negative activations plus a few images from the training data that maximally or minimally activate the channel (let's say 4 positive, 4 negative images), then you must already display more than 50 000 images.
At least we know -- thanks to Network Dissection -- that we do not need to investigate random directions.
 -->
<p>チャンネルのアクティブ化「のみ」を視覚化する場合でも、<strong>見るユニットが多くなりすぎます</strong>。 Inception V1 の場合、既に 5000 を超えるチャンネルが9つの畳み込み層からあります。 さらに負の活性化に加え、チャンネルを最大または最小に活性化する学習データからのいくつかの画像（例えば、4つの正、4つの負の画像）も表示する場合は、50000 を超える画像を表示する必要があります。 少なくとも、ネットワーク解剖のおかげで、ランダムな方向を調査する必要がないことは分かっています。</p>
<!--
**Illusion of Interpretability?**
  The feature visualizations can convey the illusion that we understand what the neural network is doing.
  But do we really understand what is going on in the neural network?
  Even if we look at hundreds or thousands of feature visualizations, we cannot understand the neural network.
  The channels interact in a complex way, positive and negative activations are unrelated, multiple neurons might learn very similar features and for many of the features we do not have equivalent human concepts.
  We must not fall into the trap of believing we fully understand neural networks just because we believe we saw that neuron 349 in layer 7 is activated by daisies.
  Network Dissection showed that architectures like ResNet or Inception have units that react to certain concepts.
  But the IoU is not that great and often many units respond to the same concept and some to no concept at all.
  They channels are not completely disentangled and we cannot interpret them in isolation.

For Network Dissection, **you need datasets that are labeled on the pixel level** with the concepts.
  These datasets take a lot of effort to collect, since each pixel needs to be labeled, which usually works by drawing segments around objects on the image.

Network Dissection only aligns human concepts with positive activations but not with negative activations of channels.
As the feature visualizations showed, negative activations seem to be linked to concepts.
This might be fixed by additionally looking at the lower quantile of activations.
 -->
<p><strong>解釈可能の錯覚</strong> 特徴の視覚化は、ニューラルネットワークが何をしているのかを理解しているという錯覚を私たちに伝える可能性があります。 しかし、ニューラルネットワークで何が起こっているのかを本当に理解していますか。 仮に数百または数千の特徴の視覚化を見ても、ニューラルネットワークは理解できません。 チャンネルは複雑な方法で相互作用し、正と負の活性化は無関係であり、複数のニューロンが非常に類似した機能を学習する可能性もあり、特徴の多くについては人間に同等の概念は存在しません。 レイヤー7のニューロン349がヒナギクによって活性化されるのを確認できたからといって、ニューラルネットワークを完全に理解していると信じる罠に陥ってはなりません。 Network Dissection は、ResNet や Inception のような設計には、特定の概念に反応するユニットがあることを示しました。 しかし、IoU はそれほど優れたものではなく、多くの場合、多くのユニットが同じ概念に反応し、一部のユニットはまったく概念に反応しません。 チャンネルは完全に解きほぐされておらず、単独で解釈できません。</p>
<p>ネットワーク解剖の場合、<strong>ピクセルレベルでラベル付けされたデータセット</strong>の概念が必要になります。 これらのデータセットは、各ピクセルにラベルを付ける必要があるため、収集に多大な労力を要します。これは通常、画像上の物体の周囲に境界線を引くことでなされます。</p>
<p>ネットワーク解剖は、人間の概念を正の活性化と一致させるだけであり、チャンネルの負の活性化とは一致させません。 特徴の視覚化が示したように、負の活性化は概念とリンクしているようです。 これは、活性化の下位分位数をさらに調べることで修正される可能性があります。</p>
</div>
<div id="ソフトウェアとその他の資料" class="section level3">
<h3><span class="header-section-number">7.1.5</span> ソフトウェアとその他の資料</h3>
<!--### Software and Further Material -->
<!--
There is an open-source implementation of feature visualization called [Lucid](https://github.com/tensorflow/lucid).
You can conveniently try it in your browser by using the notebook links that are provided on the Lucid Github page.
No additional software is required.
Other implementations are [tf_cnnvis](https://github.com/InFoCusp/tf_cnnvis) for TensorFlow, [Keras Filters](https://github.com/jacobgil/keras-filter-visualization) for Keras and [DeepVis](https://github.com/yosinski/deep-visualization-toolbox) for Caffe.

Network Dissection has a great [project website](http://netdissect.csail.mit.edu/).
Next to the publication, the website hosts additional material such as code, data and visualizations of activation masks.
-->
<p><a href="https://github.com/tensorflow/lucid">Lucid</a>と呼ばれる特徴視覚化のオープンソース実装があります。 Lucid Github ページにあるノートブックのリンクから、ウェブブラウザ上で簡単に試すことができます。 追加のソフトウェアは必要ありません。 他には Tensorflow の <a href="https://github.com/InFoCusp/tf_cnnvis">tf_cnnvis</a> 、Keras の<a href="https://github.com/jacobgil/keras-filter-visualization">Keras Filters</a>、Caffe の<a href="https://github.com/yosinski/deep-visualization-toolbox">DeepVis</a> で実装できます。 Network Dissection には素晴らしい<a href="http://netdissect.csail.mit.edu/">プロジェクトのウェブサイト</a>があります。 ウェブサイトでは出版物の隣にコード、データ、活性化マスクなどが視覚化された追加資料がホストされています。</p>

</div>
</div>
<!-- </div> -->
<div class="footnotes">
<hr />
<ol start="66">
<li id="fn66"><p>Nguyen, Anh, et al. &quot;Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.&quot; Advances in Neural Information Processing Systems. 2016.<a href="cnn-features.html#fnref66">↩</a></p></li>
<li id="fn67"><p>Nguyen, Anh, et al. &quot;Plug &amp; play generative networks: Conditional iterative generation of images in latent space.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.<a href="cnn-features.html#fnref67">↩</a></p></li>
<li id="fn68"><p>Olah, et al., &quot;Feature Visualization&quot;, Distill, 2017.<a href="cnn-features.html#fnref68">↩</a></p></li>
<li id="fn69"><p>Olah, et al., &quot;The Building Blocks of Interpretability&quot;, Distill, 2018.<a href="cnn-features.html#fnref69">↩</a></p></li>
<li id="fn70"><p>Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. &quot;Visualizing and understanding recurrent networks.&quot; arXiv preprint arXiv:1506.02078 (2015).<a href="cnn-features.html#fnref70">↩</a></p></li>
<li id="fn71"><p>Bau, David, et al. &quot;Network dissection: Quantifying interpretability of deep visual representations.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.<a href="cnn-features.html#fnref71">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="neural-networks.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="future.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/07.1-feature-visualization.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
