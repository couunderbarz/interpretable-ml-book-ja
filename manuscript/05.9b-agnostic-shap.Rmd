```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->

## SHAP (SHapley Additive exPlanations) {#shap}

`r if(is.html){only.in.html}`

<!--
SHAP (SHapley Additive exPlanations) by Lundberg and Lee (2016)[^lundberg2016] is a method to explain individual predictions.
SHAP is based on the game theoretically optimal [Shapley Values](#shapley).
-->
Lundberg and Lee (2016)[^lundberg2016]による SHAP (SHapley Additive exPlanations)は、個々の予測を説明する手法です。
SHAP はゲーム理論的に最適な [シャープレイ値](#shapley) に基づいています。

<!--
There are two reasons why SHAP got its own chapter and is not a subchapter of [Shapley values](#shapley).
First, the SHAP authors proposed KernelSHAP, an alternative, kernel-based estimation approach for Shapley values inspired by [local surrogate models](#lime).
And they proposed TreeSHAP, an efficient estimation approach for tree-based models.
Second, SHAP comes with many global interpretation methods based on aggregations of Shapley values.
This chapter explains both the new estimation approaches and the global interpretation methods.
-->
SHAP が [シャープレイ値](#shapley) 中の一節ではなく単独の章となっている理由は2つあります。
1つ目は、SHAP の作者らは KernelSHAP を提案したことです。これは[ローカルサロゲートモデル(local surrogate models)](#lime) から着想を得たカーネルベースのシャープレイ値の代替的な推定手法です。
そして、
彼らはツリーベースのモデルに対する効率的な推定手法である TreeSHAP を提案しました。
2つ目は、SHAP にはシャープレイ値の集合に基づいた多くの大域的解釈モデルが付随することです。
この章では新たな推定手法と大域的な解釈の両方を説明します。

<!--
I recommend reading the chapters on [Shapley values](#shapley) and [local models (LIME)](#lime) first.
-->
[シャープレイ値](#shapley) と [局所的モデル(LIME)](#lime) の章を先に読んでおくことをおすすめします。

### 定義
<!--
### Definition
-->

<!--
The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction.
The SHAP explanation method computes Shapley values from coalitional game theory.
The feature values of a data instance act as players in a coalition.
Shapley values tell us how to fairly distribute the "payout" (= the prediction) among the features.
A player can be an individual feature value, e.g. for tabular data.
A player can also be a group of feature values.
For example to explain an image, pixels can be grouped to super pixels and the prediction distributed among them.
One innovation that SHAP brings to the table is that the Shapley value explanation is represented as an additive feature attribution method, a linear model.
That view connects LIME and Shapley Values.
SHAP specifies the explanation as: 
-->
SHAP の目標は、それぞれの特徴量の予測への貢献度を計算することで、あるインスタンス x に対する予測を説明することです。
SHAP による説明では、協力ゲーム理論によるシャープレイ値を計算します。
インスタンスの特徴量の値は、協力するプレイヤーの一員として振る舞います。
シャープレイ値は、"報酬" (=予測) を特徴量間で公平に分配するにはどうしたら良いかを教えてくれます。
各プレイヤーは、例えば表形式データでは、個別の特徴量の値となります。
プレイヤーは特徴量の値の組の可能性もあります。
画像を説明する例では、画素はスーパーピクセルとしてグループ化され、予測はそれらの間で分配されるでしょう。
SHAP が生んだ革新の1つは、シャープレイ値による説明が、線形モデルのような特徴量の効果の総和として表されることです。
この観点は、LIMEとシャープレイ値を結びつけます。
SHAPは、説明を次のように記述します。

$$g(z')=\phi_0+\sum_{j=1}^M\phi_jz_j'$$

<!--
where g is the explanation model, $z'\in\{0,1\}^M$ is the coalition vector, M is the maximum coalition size and $\phi_j\in\mathbb{R}$ is the feature attribution for a feature j, the Shapley values.
What I call "coalition vector" is called "simplified features" in the SHAP paper.
I think this name was chosen, because for e.g. image data, the images are not represented on the pixel level, but aggregated to super pixels.
I believe it is helpful to think about the z's as describing coalitions:
In the coalition vector, an entry of 1 means that the corresponding feature value is "present" and 0 that it is "absent".
This should sound familiar to you if you know about Shapley values.
To compute Shapley values, we simulate that only some features values are playing ("present") and some are not ("absent").
The representation as a linear model of coalitions is a trick for the computation of the $\phi$'s.
For x, the instance of interest, the coalition vector x' is a vector of all 1's, i.e., all feature values are "present".
The formula simplifies to:
-->

ここで、g は説明モデル、$z'\in\{0,1\}^M$ は連合ベクトル、 M は連合サイズの最大値、そして $\phi_j\in\mathbb{R}$ は特徴量 j についての特徴量の属性であり、シャープレイ値です。
私が "連合ベクトル" と呼んでいるものは、SHAP の論文では "simplified features" と呼ばれています。
この名前が選ばれたのは、例えば画像データでは、画像は画素レベルではなく、スーパーピクセルに集約されたレベルで表されるためであると私は考えています。
私は z が連合を表すものであると考えるのは有用であると信じています。
連合ベクトルにおいて、要素が 1 のとき対応する特徴量が "存在" することを意味し、0 は "不在" であることを表します。
あなたがシャープレイ値について知っているならば、これは馴染み深く感じるでしょう。
シャープレイ値を計算するために、いくつかの特徴量だけがゲームに参加 ("存在") し、その他は参加しない ("不在") としてシミュレーションします。
連合の線形モデルとする表現は、$\phi$ を計算するためのトリックです。
興味があるインスタンス x に対し、連合ベクトル x' は全てが1であるベクトル、つまり、全ての特徴量が"存在"となります。
このとき、式は次のように単純になります。

$$g(x')=\phi_0+\sum_{j=1}^M\phi_j$$

<!--
You can find this formula in similar notation in the [Shapley value](#shapley) chapter.
More about the actual estimation comes later.
Let us first talk about the properties of the $\phi$'s before we go into the details of their estimation.
-->
この式は[シャープレイ値](#shapley)での表記に似ています。
実際の推定については後ほど触れます。
推定の詳細の前に、まずは $\phi$ の性質について紹介します。

<!--以下元々コメントアウトされていました-->
<!-- Desirable properties -->

<!--
Shapley values are the only solution that satisfies properties of Efficiency, Symmetry, Dummy and Additivity.
SHAP also satisfies these, since it computes Shapley values.
In the SHAP paper, you will find discrepancies between SHAP properties and Shapley properties.
SHAP describes the following three desirable properties:
-->
シャープレイ値は効率性 (Efficiency)、対称性 (Symmetry)、ダミー性 (Dummy)、加法性 (Additivity) を満たす唯一の解決案です。
シャープレイ値を計算するため、SHAP もこれらの性質を満たします。
SHAPの論文では、SHAPの性質とシャープレイ値の性質間の食い違いに気づくでしょう。
SHAPは次の望ましい3つの性質を説明します。

<!--
**1) Local accuracy**
-->
**1) 局所正確性 (Local accuracy)**

$$f(x)=g(x')=\phi_0+\sum_{j=1}^M\phi_jx_j'$$

<!--
If you define $\phi_0=E_X(\hat{f}(x))$ and set all $x_j'$ to 1, this is the Shapley efficiency property.
Only with a different name and using the coalition vector.
-->
もし $\phi_0=E_X(\hat{f}(x))$ と定義し、全ての $x_j'$ を 1 とするとき、これは、シャープレイの効率性となります。
名前が異なり、連合ベクトルを使っているだけです。

$$f(x)=\phi_0+\sum_{j=1}^M\phi_jx_j'=E_X(\hat{f}(X))+\sum_{j=1}^M\phi_j$$

<!--
**2) Missingness**
-->
**2)欠損 (Missingness)**

$$x_j'=0\Rightarrow\phi_j=0$$

<!--
Missingness says that a missing feature gets an attribution of zero.
Note that $x_j'$ refers to the coalitions, where a value of 0 represents the absence of a feature value.
In coalition notation, all feature values $x_j'$ of the instance to be explained should be '1'.
The presence of a 0 would mean that the feature value is missing for the instance of interest.
This property is not among the properties of the "normal" Shapley values.
So why do we need it for SHAP?
Lundberg calls it a ["minor book-keeping property"](https://github.com/slundberg/shap/issues/175#issuecomment-407134438).
A missing feature could -- in theory -- have an arbitrary Shapley value without hurting the local accuracy property, since it is multiplied with $x_j'=0$.
The Missingness property enforces that missing features get a Shapley value of 0. 
In practice this is only relevant for features that are constant.
-->
欠損は、欠損している特徴量の属性がゼロになることを意味しています。
$x_j'$ は連合を指し、値が 0 のとき、特徴量が不在を示していることに注意してください。
連合の表記では、説明されるインスタンスの全ての特徴量 $x_j'$ は 1 である必要があります。
0 の存在は、注目しているインスタンスの特徴量の値が欠損していることを意味します。
この性質は、"普通の"シャープレイ値の性質には含まれていません。
では、なぜこれがSHAPのために必要なのでしょうか。
Lundberg は["minor book-keeping property"](https://github.com/slundberg/shap/issues/175#issuecomment-407134438) と呼んでいます。
欠損している特徴量は、理論的に、 $x_j'=0$ と掛け合わされるため、局所的な精度を損なうことなく、任意のシャープレイ値を持つ事ができます。
欠損の性質は、欠損している特徴量は 0 のシャープレイ値を持つようにします。
実際には、これは定数の特徴量にだけ関係します。

<!--
**3)  Consistency**
-->
**3) 一貫性 (Consistency)**

<!--
Let $f_x(z')=f(h_x(z'))$ and $z_{\setminus{}j'}$ indicate that $z_j'=0$.
For any two models f and f' that satisfy:

$$f_x'(z')-f_x'(z_{\setminus{}j}')\geq{}f_x(z')-f_x(z_{\setminus{}j}')$$

for all inputs $z'\in\{0,1\}^M$, then:

$$\phi_j(f',x)\geq\phi_j(f,x)$$

The consistency property says that if a model changes so that the marginal contribution of a feature value increases or stays the same (regardless of other features), the Shapley value also increases or stays the same.
From Consistency the Shapley properties Linearity, Dummy and Symmetry follow, as described in the Appendix of Lundberg and Lee.
-->
$f_x(z')=f(h_x(z'))$ と $z_{\setminus{}j'}$ は $z_j'=0$ を表すとします。
任意の2つのモデル f と f' に対して、任意の入力 $z'\in\{0,1\}^M$ が、$$f_x'(z')-f_x'(z_{\setminus{}j}')\geq{}f_x(z')-f_x(z_{\setminus{}j}')$$を満たすならば、$$\phi_j(f',x)\geq\phi_j(f,x)$$が成り立ちます。

一貫性は、もし特徴量の値の周辺寄与が（他の特徴量に関わらず）増加または同じままモデルが変化すると、シャープレイ値もまた、増加または同じままになるということを言っています。
Lundberg と Leeの付録で説明されている通り、一貫性から線形性、ダミー性、対称性が導かれます。

### KernelSHAP
<!--
### KernelSHAP

-->
<!--以下元々コメントアウトされていました-->
<!-- The general Idea of linear model -->

<!--
KernelSHAP estimates for an instance x the contributions of each feature value to the prediction.
KernelSHAP consists of 5 steps:

- Sample coalitions $z_k'\in\{0,1\}^M,\quad{}k\in\{1,\ldots,K\}$ (1 = feature present in coalition, 0 = feature absent).
- Get prediction for each $z_k'$ by first converting $z_k'$ to the original feature space and then applying model f: $f(h_x(z_k'))$
- Compute the weight for each $z_k'$ with the SHAP kernel.
- Fit weighted linear model.
- Return Shapley values $\phi_k$, the coefficients from the linear model.
-->
KernelSHAP はインスタンス x の予測に対するそれぞれの特徴量の値の寄与を推定します。
KernelSHAP は以下の5つのステップで構成されています。

- 連合 $z_k'\in\{0,1\}^M,\quad{}k\in\{1,\ldots,K\}$ (1 = 連合内に特徴量が存在する, 0 =特徴量が存在しない) をサンプリングする。
- 最初に、$z_k'$ を元の特徴空間に変換し、モデル  f: $f(h_x(z_k'))$ を適用しそれぞれの $z_k'$ を予測する。
- SHAP カーネルを使って各 $z_k'$ の重みを計算する。
- 重み付きの線形モデルで学習する。
- シャープレイ値 $\phi_k$ と 線形モデルの係数を返す。

<!--
We can create a random coalition by repeated coin flips until we have a chain of 0's and 1's.
For example, the vector of (1,0,1,0) means that we have a coalition of the first and third features.
The K sampled coalitions become the dataset for the regression model.
The target for the regression model is the prediction for a coalition.
("Hold on!," you say, "The model has not been trained on these binary coalition data and can't make predictions for them.")
To get from coalitions of feature values to valid data instances, we need a function $h_x(z')=z$ where $h_x:\{0,1\}^M\rightarrow\mathbb{R}^p$.
The function $h_x$ maps 1's to the corresponding value from the instance x that we want to explain.
For tabular data, it maps 0's to the values of another instance that we sample from the data.
This means that we equate "feature value is absent" with "feature value is replaced by random feature value from data".
For tabular data, the following figure visualizes the mapping from coalitions to feature values:
-->
0 と 1 の連鎖ができるまでコイントスを繰り返す事によってランダムな連合を作成できます。
例えば、(1,0,1,0) のベクトルは、1番目と3番目の特徴量の連合を意味します。
K 個のサンプルされた連合は線形モデルのためのデータセットになります。
回帰モデルの目標は連合に対する予測です。
このモデルは、バイナリの連合ベクトルに対して学習されていないため予測はできないと思うかもしれません。
特徴量の連合から有効なデータインスタンスを取得するため、関数 $h_x(z')=z$ (ただし、$h_x:\{0,1\}^M\rightarrow\mathbb{R}^p$) が必要です。
関数 $h_x$ は 1 を説明したいインスタンス x に対する値に割り当てます。
表形式のデータの場合、0 をサンプリングした他のインスタンスの値に割り当てます。
これは、"特徴量の値が不在"であるということと、"特徴量の値は、データからランダムで選ばれた特徴量で置き換えられる"ということが等価であることを意味しています。
表形式データの場合、以下の図は連合から特徴の値に変換する方法を可視化しています。

<!--
fig.cap = "Function $h_x$ maps a coalition to a valid instance. For present features (1), $h_x$ maps to the feature values of x. For absent features (0), $h_x$ maps to the values of a randomly sampled data instance."
-->

```{r shap-simplified-feature, fig.cap = "関数 $h_x$ は、連合から有効なインスタンスへの写像。特徴量が存在 (1) するとき、$h_x$ は x の特徴量の値に変換する。特徴量が不在 (0) のとき、$h_x$ はランダムにサンプリングされたデータインスタンスの値に変換する。", out.width=800}
knitr::include_graphics("images/shap-simplified-features.jpg")
```

<!--
$h_x$ for tabular data treats $X_C$ and $X_S$ as independent and integrates over the marginal distribution:
-->
表形式データに対する $h_x$ は、$X_C$ と $X_S$ は独立として扱い、周辺分布上で積分します。

$$f(h_x(z'))=E_{X_C}[f(x)]$$

<!--
Sampling from the marginal distribution means ignoring the dependence structure between present and absent features.
KernelSHAP therefore suffers from the same problem as all permutation-based interpretation methods.
The estimation puts too much weight on unlikely instances.
Results can become unreliable.
But it is necessary to sample from the marginal distribution.
If the absent feature values would be sampled from the conditional distribution, then the resulting values are no longer Shapley values.
The resulting values would violate the Shapley axiom of Dummy, which says that a feature that does not contribute to the outcome should have a Shapley value of zero.

For images, the following figure describes a possible mapping function:
-->
周辺分布からのサンプリングは、存在する特徴量と存在しない特徴量の間の依存関係を無視することを意味します。
それゆえに、KernelSHAP は permutation ベースの解釈手法と同じ問題があります。
推定は、現実的に起こりそうもないインスタンスに過剰な重みを与えます。
結果は信頼できないものになります。
しかし、周辺分布からサンプリングすることは必要です。
もし、存在しない特徴量が条件付き分布からサンプルされた場合、結果の値はもはやシャープレイ値ではありません。
結果の値は、結果に寄与しない特徴量のシャープレイ値は 0 であるという、ダミーに関するシャープレイの公理に反します。

画像の場合、以下の図が、可能なマッピング関数を説明します。

<!--
fig.cap = "Function $h_x$ maps coalitions of super pixels (sp) to images. Super-pixels are groups of pixels. For present features (1), $h_x$ returns the corresponding part of the original image. For absent features (0), $h_x$ greys out the corresponding area. Assigning the average color of surrounding pixels or similar would also be an option."
-->

```{r fig.cap = "関数 $h_x$ はスーパーピクセル(sp)の連合を画像にマッピングします。存在する特徴量(1)について、$h_x$ は元の画像の相当する部分を返します。不在の特徴量(0)については、$h_x$ は相当する部分をグレーアウトします。周りのピクセルの平均の色を割り当てるか似たような色で割り当てるかは任意です。", out.width=800}
knitr::include_graphics("images/shap-superpixel.jpg")
```

<!--以下元々コメントアウトされていました-->
<!-- Kernel -->

<!--
The big difference to LIME is the weighting of the instances in the regression model.
LIME weights the instances according to how close they are to the original instance.
The more 0's in the coalition vector, the smaller the weight in LIME.
SHAP weights the sampled instances according to the weight the coalition would get in the Shapley value estimation.
Small coalitions (few 1's) and large coalitions (i.e. many 1's) get the largest weights.
The intuition behind it is:
We learn most about individual features if we can study their effects in isolation.
If a coalition consists of a single feature, we can learn about the features' isolated main effect on the prediction.
If a coalition consists of all but one feature, we can learn about this features' total effect (main effect plus feature interactions).
If a coalition consists of half the features, we learn little about an individual features contribution, as there are many possible coalitions with half of the features.
To achieve Shapley compliant weighting, Lundberg et. al propose the SHAP kernel:
-->
LIME との大きな違いは回帰モデルの中のインスタンスの重みです。
LIME は、元のインスタンスにどのくらい近いかによってインスタンスの重みを決定します。
連合ベクトルの中の 0 が多いほど、LIME の重みは小さくなります。
SHAP は、連合がシャープレイ値の推定で得るであろう重みに従って、サンプリングされたインスタンスに重み付けをします。
小さな連合（1が少ない）と大きな連合（1が多い）が最も大きな重みをとります。
この理由は以下の通りです。
もし個々の特徴量の影響を個別に調べることができるのであれば、個々の特徴量のほとんどを知ることになります。
もし連合が単一の特徴量で構成されている場合、予測に対する特徴量の個々の主な影響を知ることができます。
もし連合が1つを除いたすべての特徴量を基に構成されているなら、特徴量の全体効果(主要な効果+特徴量の相互作用)を知ることができます。
もし連合が特徴量の半分で構成されている場合、半分の特徴量となりうる連合が多くあるため、個々の特徴量についてほとんど知ることができません。
シャープレイ準拠の重みを達成するために、Lundbergらは、SHAP カーネルを提案しました。

$$\pi_{x}(z')=\frac{(M-1)}{\binom{M}{|z'|}|z'|(M-|z'|)}$$

<!--
Here, M is the maximum coalition size and $|z'|$ the number of present features in instance z'.
Lundberg and Lee show that linear regression with this kernel weight yields Shapley values.
If you would use the SHAP kernel with LIME on the coalition data, LIME would also estimate Shapley values!
-->
ここで、M は連合の最大サイズ、 $|z'|$ はインスタンス z' 内に存在する特徴量の数です。
Lundberg と Lee はこのカーネルの重みを使った線形回帰でシャープレイ値が得られることを示しています。
もし連合データにおいてSHAPカーネルをLIMEで使用する場合、LIMEもまたシャープレイ値を推定します。

<!--以下元々コメントアウトされていました-->
<!-- Sampling trick -->

<!--
We can be a bit smarter about the sampling of coalitions:
The smallest and largest coalitions take up most of the weight.
We get better Shapley value estimates by using some of the sampling budget K to include these high-weight coalitions instead of sampling blindly.
We start with all possible coalitions with 1 and M-1 features, which makes 2 times M coalitions in total.
When we have enough budget left (current budget is K - 2M), we can include coalitions with two features and with M-2 features and so on.
From the remaining coalition sizes, we sample with readjusted weights.
-->
連合のサンプリングに関してもう少し賢い方法があります。
最も小さい連合と大きい連合が重みのほとんどを奪います。
盲目的にサンプリングをする代わりに、サンプリング予算 K の一部使ってこれらの高い重みの連合を含めることで、よりよいシャープレイ値の推定を得ることができます。
そこで、1個 と Mー1個の特徴量を持つ可能な全ての連合(全部で 2M 個の連合)からスタートします。 
十分な予算が残っている時(現在の予算は K - 2M)、2個の特徴量の連合や、M-2個の特徴量の連合などを含めます。
残りのサイズの連合からサンプリングを行い、重みを再調整します。

<!--以下元々コメントアウトされていました-->
<!-- Linear Model -->

<!--
We have the data, the target and the weights.
Everything to build our weighted linear regression model:
-->
ターゲットと重みのデータを持っています。
重み付き線形回帰モデルを構築する全ては以下の通りです。

$$g(z')=\phi_0+\sum_{j=1}^M\phi_jz_j'$$

<!--
We train the linear model g by optimizing the following loss function L:
-->
線形モデル g を損失関数 L を最適化することによって学習します。

$$L(f,g,\pi_{x})=\sum_{z'\in{}Z}[f(h_x(z'))-g(z')]^2\pi_{x}(z')$$

<!--
where Z is the training data.
This is the good old boring sum of squared errors that we usually optimize for linear models.
The estimated coefficients of the model, the $\phi_j$'s are the Shapley values.

Since we are in a linear regression setting, we can also make use of the standard tools for regression.
For example, we can add regularization terms to make the model sparse.
If we add an L1 penalty to the loss L, we can create sparse explanations.
(I am not so sure whether the resulting coefficients would still be valid Shapley values though)
-->
ただし、Z は学習用データです。
これは、線形モデルを最適化するための、古き良き、退屈な二乗誤差の合計です。
予測されたモデルの係数  $\phi_j$ がシャープレイ値です。

線形回帰の設定なので、回帰のための標準ツールも使用できます。
例えば、正則化項を追加し、モデルをスパース化できます。
もし、私たちが損失 L に L1 ペナルティを追加することで、スパースな説明ができます。
（ただし、スパース化によって、有効なシャープレイ値のままであるかどうか確かでありません。）

### TreeSHAP
<!--### TreeSHAP-->

<!--
Lundberg et. al (2018)[^tree-shap] proposed TreeSHAP, a variant of SHAP for tree-based machine learning models such as decision trees, random forests and gradient boosted trees.
TreeSHAP was introduced as a fast, model-specific alternative to KernelSHAP, but it turned out that it can produce unintuitive feature attributions.
-->
Lundberg ら(2018)[^tree-shap] は、決定木やランダムフォレスト、GBT(gradient boosted trees)などといった木構造ベースの機械学習モデルのための SHAP のバリエーションとして、TreeSHAP を提案しました。
TreeSHAP は高速でモデルに特化した KernelSHAP の代替として導入されましたが、直感的でない 特徴量の属性を生成しうることが判明しました。

<!--
TreeSHAP defines the value function using the conditional expectation $E_{X_S|X_C}(f(x)|x_S)$ instead of the marginal expectation.
The problem with the conditional expectation is that features that have no influence on the prediction function f can get a TreeSHAP estimate different from zero.[^dummy1][^dummy2]
The non-zero estimate can happen when the feature is correlated with another feature that actually has an influence on the prediction.
-->
TreeSHAP は、周辺分布の期待値の代わりに条件付き期待値 $E_{X_S|X_C}(f(x)|x_S)$ を使って関数の値を定義します。
条件付き期待値の問題点は、予測関数 f に影響を与えない特徴量が、非ゼロの TreeSHAP 推定値を取得しうることです。[^dummy1][^dummy2]
非ゼロの推定値は、特徴量が実際に予測に影響する他の特徴量と相関を持つときに起こります。

<!--
How much faster is TreeSHAP?
Compared to exact KernelSHAP, it reduces the computational complexity from $O(TL2^M)$ to $O(TLD^2)$, where T is the number of trees, L is the maximum number of leaves in any tree and D the maximal depth of any tree.
-->
TreeSHAPはどれほど高速なのでしょうか。
厳密なKernelSHAPと比較して、T を木の数、L を木の中の最大の葉の数、D を木の中の最大の深さとしたとき、計算量を $O(TL2^M)$ から $O(TLD^2)$ にまで削減できます。

<!--以下元々コメントアウトされていました-->
<!-- To explain an individual prediction with exact Shapley values, we have to estimate  $E(f(x)|x_S)$ for all possible feature value subsets S.-->
<!--厳密なシャープレイ値を使って個別の予測を説明するためには、あり得る全ての特徴量の部分集合 S に対して $E(f(x)|x_S)$ を推定しなければなりません。-->

<!--
TreeSHAP uses the conditional expectation $E_{X_S|X_C}(f(x)|x_S)$ to estimate effects.
I will give you some intuition on how we can compute the expected prediction for a single tree, an instance x and feature subset S.
If we conditioned on all features -- if S was the set of all features -- then the prediction from the node in which the instance x falls would be the expected prediction.
If we did no condition on any feature -- if S was empty -- we would use the weighted average of predictions of all terminal nodes.
If S contains some, but not all, features, we ignore predictions of unreachable nodes.
Unreachable means that the decision path that leads to this node contradicts values in $x_S$.
From the remaining terminal nodes, we average the predictions weighted by node sizes (i.e. number of training samples in that node).
The mean of the remaining terminal nodes, weighted by the number of instances per node, is the expected prediction for x given S.
The problem is that we have to apply this procedure for each possible subset S of the feature values.
-->
TreeSHAPは影響を推定するために条件付き期待値 $E_{X_S|X_C}(f(x)|x_S)$ を使用します。
あるインスタンス x と特徴量の部分集合 S に対して、1つの木の期待される予測値がどのように計算されるのか、直感的に理解してみましょう。
全ての特徴量で条件付けされているとき -- S は全ての特徴量の集合 -- 、期待される予測は、インスタンス x が含まれるノードの予測値になります。
どの特徴量でも条件付けされていないなら -- S は空集合 -- 、全ての終端ノードの予測の重み付き平均が期待される予測になります。
いくつかの特徴量を S が含んでいる場合(全てではない)、到達できないノードの予測は無視します。
ちなみに、到達できないとは、ノードへの決定経路(decision path)が、$x_S$ の値と矛盾することを意味します。
残った終端ノードから、ノードサイズ（ノード内の学習サンプル数）によって重みづけられた予測値の平均を計算します。
この結果が S が与えられた時の x に対する期待される予測値となります。
問題点は、この手順をありうる特徴量の部分集合 S のそれぞれに対して適用しなければならないことです。

<!--以下元々コメントアウトされていました-->
<!--
This means $\sum_{i=1}{p}\frac{(p-i)!i!}{i!}$ times.
Here, each summand is the set of all possible subsets S with the same cardinality (e.g. all possible subsets with 2 features).
-->
<!--
これは $\sum_{i=1}{p}\frac{(p-i)!i!}{i!}$ 回になります。
ここで、それぞれの被加数は同じ濃度の全てのあり得る部分集合（例えば、特徴量2つによる全てのあり得る部分集合） S です。
-->

<!--
TreeSHAP computes in polynomial time instead of exponential.
The basic idea is to push all possible subsets S down the tree at the same time.
For each decision node we have to keep track of the number of subsets.
This depends on the subsets in the parent node and the split feature.
For example, when the first split in a tree is on feature x3, then all the subsets that contain feature x3 will go to one node (the one where x goes).
Subsets that do not contain feature x3 go to both nodes with reduced weight.
Unfortunately, subsets of different sizes have different weights.
The algorithm has to keep track of the overall weight of the subsets in each node.
This complicates the algorithm.
I refer to the original paper for details of TreeSHAP.
The computation can be expanded to more trees:
Thanks to the Additivity property of Shapley values, the Shapley values of a tree ensemble is the (weighted) average of the Shapley values of the individual trees.
-->
TreeSHAP は指数時間ではなく多項式時間で計算します。
基本的な考えは、全てのあり得る部分集合 S を同時に木へ押し込むことです。
それぞれの決定ノードに対し、部分集合の数を追跡する必要があります。
これは、親ノードの部分集合と特徴量の分割に依存します。
例えば、ある木における最初の分割が x3 とすると、全ての x3 を含む部分集合が1つのノード（xの行先）へ行きます。
x3 を含まない部分集合は、削減された重みと共に両方のノードへ行きます。
不運にも、異なるサイズの部分集合は異なる重みを持ちます。
アルゴリズムはそれぞれのノードにおける部分集合の重み全体を追跡しなければなりません。
これがアルゴリズムを複雑にしています。
TreeSHAP の詳細について、原論文を参照してください。
この計算は複数の木に対しても拡張されます。 
シャープレイ値の加法性のおかげで、アンサンブル木のシャープレイ値は、個別の木のシャープレイ値の (加重) 平均となります。

<!--
Next, we will look at SHAP explanations in action.
-->
次に、SHAP による説明を見てみましょう。

### 例
<!--
### Examples
-->

<!--
I trained a random forest classifier with 100 trees to predict the [risk for cervical cancer](#cervical).
We will use SHAP to explain individual predictions.
We can use the fast TreeSHAP estimation method instead of the slower KernelSHAP method, since a random forest is an ensemble of trees.
But instead of relying on the conditional distribution, this example uses the marginal distribution.
This is described in the package, but not in the original paper.
The Python TreeSHAP function is slower with the marginal distribution, but still faster than KernelSHAP, since it scales linearly with the rows in the data.
-->
[子宮頸がんのリスク](#cervical)を予測するために100の木を持つランダムフォレスト分類器を学習させました。
それぞれの予測を説明するために SHAP を使います。ランダムフォレストは木のアンサンブルであるため、Kernel SHAP の代わりに高速な TreeSHAP が使用できます。
しかし、この例では条件付き分布の代わりに周辺分布を用いています。これはパッケージで説明されており、元の論文では説明されていません。
Python の TreeSHAP 関数は周辺分布では低速ですが KernelSHAP よりは高速です、なぜならデータの行に比例して増加していくからです。

<!--
Because we use the marginal distribution here, the interpretation is the same as in the [Shapley value chapter](#shapley).
But with the Python shap package comes a different visualization:
You can visualize feature attributions such as Shapley values as "forces".
Each feature value is a force that either increases or decreases the prediction.
The prediction starts from the baseline.
The baseline for Shapley values is the average of all predictions.
In the plot, each Shapley value is an arrow that pushes to increase (positive value) or decrease (negative value) the prediction.
These forces balance each other out at the actual prediction of the data instance.
-->
ここでは周辺分布を使っているので、説明は[シャープレイ値の章](#shapley)と同じです。
しかし、Python の shap パッケージは異なる可視化になります。
特徴量の属性をシャープレイ値のような "力(forces)" として可視化できます。
それぞれの特徴量の値は予測値を増加させるか減少させるかの力を持ちます。
予測値は基準値から始まります。
シャープレイ値の基準値はすべての予測値の平均になります。
プロットでは、シャープレイ値は予測値を増加させる方向(正の値)か減少させる方向(負の値)を指す矢印となります。
この影響はデータインスタンスの実際の予測値で互いに釣り合っています。

<!--
The following figure shows SHAP explanation force plots for two women from the cervical cancer dataset:
-->
次の図は、子宮頸がんデータセットからの2人の女性の SHAP による説明の様子を示しています。

<!--
fig.cap = "SHAP values to explain the predicted cancer probabilities of two individuals. The baseline -- the average predicted probability -- is 0.066. The first woman has a low predicted risk of 0.06. Risk increasing effects such as STDs are offset by decreasing effects such as age. The second woman has a high predicted risk of 0.71. Age of 51 and 34 years of smoking increase her predicted cancer risk."
-->

```{r, fig.cap = "2人の個人の予測される癌の確率を説明するためのSHAP。 ベースライン（平均予測確率）は 0.066 であり、 最初の女性の予測リスクは 0.06 と低くなっている。性感染症（STD）などのリスク増加効果は、年齢などの減少効果によって相殺される。2人目の女性の予測リスクは 0.71 と高くなっている。51歳と34年間の喫煙は、癌の予測リスクを高める。", out.width = 800}
img1 <-  rasterGrob(as.raster(png::readPNG("images/shap-explain-1.png")), interpolate = FALSE)
img2 <-  rasterGrob(as.raster(png::readPNG("images/shap-explain-2.png")), interpolate = FALSE)
grid.arrange(img1, img2, ncol = 1)
```

<!--
These were explanations for individual predictions.

Shapley values can be combined into global explanations.
If we run SHAP for every instance, we get a matrix of Shapley values.
This matrix has one row per data instance and one column per feature.
We can interpret the entire model by analyzing the Shapley values in this matrix.

We start with SHAP feature importance.
-->
これらは、個々の予測に対する説明でした。

シャープレイ値は、グローバルな説明のために組み合わせることができます。
すべてのインスタンスに対して SHAP を実行すると、シャープレイ値の行列が得られます。
この行列は、データインスタンスごとに1つの行があり、特徴量ごとに1つの列があります。
この行列のシャープレイ値を分析することで、モデル全体を解釈できます。

SHAPを使った特徴量重要度から始めます。

### SHAP 特徴量重要度 (SHAP Feature Importance)
<!--
### SHAP Feature Importance
-->

<!--
The idea behind SHAP feature importance is simple:
Features with large absolute Shapley values are important.
Since we want the global importance, we average the absolute Shapley values per feature across the data:
-->
SHAP 特徴量重要度の背後にあるアイデアは単純です。
シャープレイ値の絶対値が大きい特徴量は重要です。
大域的な重要度を求めたいため、特徴量ごとのシャープレイ値の絶対値のデータ内での平均をとります。

$$I_j=\sum_{i=1}^n{}|\phi_j^{(i)}|$$

<!--
Next, we sort the features by decreasing importance and plot them.
The following figure shows the SHAP feature importance for the random forest trained before for predicting cervical cancer.
-->
次に、重要度の降順に特徴量を並べ替え、それらをプロットします。
次の図は、以前子宮頸がんの予測のために学習したランダムフォレストの SHAP 特徴量重要度です。

<!--
fig.cap="SHAP feature importance measured as the mean absolute Shapley values. The number of years with hormonal contraceptives was the most important feature, changing the predicted absolute cancer probability on average by 2.4 percentage points (0.024 on x-axis)."
-->

```{r fig.cap="シャープレイ値の絶対値の平均として計算された SHAP 特徴量重要度。ホルモン避妊薬の使用年数は最も重要な特徴量であり、予測される癌の可能性の絶対値を2.4%変動させます (x軸での0.024).", out.width=800}
knitr::include_graphics("images/shap-importance.png")
```

<!--
SHAP feature importance is an alternative to [permutation feature importance](#feature-importance).
There is a big difference between both importance measures:
Permutation feature importance is based on the decrease in model performance.
SHAP is based on magnitude of feature attributions.
-->
SHAP 特徴量重要度は [permutation feature importance](#feature-importance) の代替手法です。
ただし、これらの重要度の計測には大きな違いがあります。
Permutation feature importance はモデルの性能低下に基づきます。
一方で、SHAP は特徴量の帰属の大きさに基づいています。

<!--
The feature importance plot is useful, but contains no information beyond the importances.
For a more informative plot, we will next look at the summary plot.
-->
特徴量重要度プロットは有用ですが、重要度以上の情報は含んでいません。
より有益なプロットとして、次の summary plot があります。

### SHAP Summary Plot
<!--
### SHAP Summary Plot
-->

<!--
The summary plot combines feature importance with feature effects.
Each point on the summary plot is a Shapley value for a feature and an instance.
The position on the y-axis is determined by the feature and on the x-axis by the Shapley value.
The color represents the value of the feature from low to high.
Overlapping points are jittered in y-axis direction, so we get a sense of the distribution of the Shapley values per feature.
The features are ordered according to their importance.
-->
この summary plot は、特徴量重要度と特徴量の影響を結びつけます。
Summary plot の各点はあるインスタンスの特徴量のシャープレイ値です。
y軸方向の位置は特徴量によって、x軸方向の位置はシャープレイ値によって決まります。
色は特徴量の値の大小を表します。
特徴量ごとのシャープレイ値の分布を知ることができるように、重複点はy軸方向にずらされています。
特徴量は重要度に従って並べられます。

<!--
fig.cap = "SHAP summary plot. Low number of years on hormonal contraceptives reduce the predicted cancer risk, a large number of years increases the risk. Your regular reminder: All effects describe the behavior of the model and are not necessarily causal in the real world."
-->

```{r fig.cap = "SHAP summary plot. ホルモン避妊薬使用年数が低いと予測される癌のリスクは低減し、高いとリスクが増加する。ただし、あらゆる効果はモデルの振る舞いを説明するものであり、必ずしも実世界での要因とは限らないことを注意。",  out.width=800}
knitr::include_graphics("images/shap-importance-extended.png")
```

<!--
In the summary plot, we see first indications of the relationship between the value of a feature and the impact on the prediction.
But to see the exact form of the relationship, we have to look at SHAP dependence plots.
-->
Summary plotでは、特徴量の値と予測への重要度の関係性を示しています。
しかし、関係性の正確な姿を見るには、SHAP dependence plot を見る必要があります。

### SHAP Dependence Plot
<!--
### SHAP Dependence Plot
-->

<!--
SHAP feature dependence might be the simplest global interpretation plot:
1) Pick a feature.
2) For each data instance, plot a point with the feature value on the x-axis and the corresponding Shapley value on the y-axis.
3) Done.
-->
SHAP feature dependence は、最も単純な大域的な解釈のためのプロットかもしれません。
1) 特徴量を選ぶ。
2) それぞれのインスタンスに対して、x軸に特徴量の値を、y軸に対応するシャープレイ値をプロットします。
3) おわり。

<!--
Mathematically, the plot contains the following points: $\{(x_j^{(i)},\phi_j^{(i)})\}_{i=1}^n$
-->
数学的には、プロットは次の点を含みます。

$\{(x_j^{(i)},\phi_j^{(i)})\}_{i=1}^n$

<!--
The following figure shows the SHAP feature dependence for years on hormonal contraceptives:
-->
ホルモン避妊薬使用年数の SHAP feature dependence を以下に示します。

<!--
fig.cap="SHAP dependence plot for years on hormonal contraceptives. Compared to 0 years, a few years lower the predicted probability and a high number of years increases the predicted cancer probability."
-->

```{r fig.cap="ホルモン避妊薬使用年数の SHAP feature dependence。 0年と比較して、低い年数のときは予測される確率を下げ、高い年数は予測される確率を増加させている。", out.width=800}
knitr::include_graphics("images/shap-dependence.png")
```

<!--
SHAP dependence plots are an alternative to [partial dependence plots](#pdp) and [accumulated local effects](#ale).
While PDP and ALE plot show average effects, SHAP dependence also shows the variance on the y-axis.
Especially in case of interactions, the SHAP dependence plot will be much more dispersed in the y-axis.
The dependence plot can be improved by highlighting these feature interactions.
-->
SHAP dependence plot は、[partial dependence plots](#pdp) や [accumulated local effects](#ale) の代替手法です。
PDP や ALE plot が平均効果を示すのに対して、SHAP dependence はy軸方向のばらつきも示せます。
特に相互作用がある場合には、SHAP dependence plot はy軸方向に更にばらつくでしょう。
SHAP dependence plot はそれらの特徴量の相互作用を強調表示することで、改善されます。

### SHAP 相互作用値 (SHAP Interaction Values)
<!--
### SHAP Interaction Values
-->

<!--
The interaction effect is the additional combined feature effect after accounting for the individual feature effects.
The Shapley interaction index from game theory is defined as:
-->
相互作用効果は、個々の特徴量の影響を考慮した後の追加の複合的な特徴量の効果です。
ゲーム理論から、シャープレイ相互作用は下記のように定義できます。

$$\phi_{i,j}=\sum_{S\subseteq\setminus\{i,j\}}\frac{|S|!(M-|S|-2)!}{2(M-1)!}\delta_{ij}(S)$$

<!--
when $i\neq{}j$ and:
-->
$i\neq{}j$ のときであり、$$\delta_{ij}(S)=f_x(S\cup\{i,j\})-f_x(S\cup\{i\})-f_x(S\cup\{j\})+f_x(S)$$ です。

<!--
This formula subtracts the main effect of the features so that we get the pure interaction effect after accounting for the individual effects.
We average the values over all possible feature coalitions S, as in the Shapley value computation.
When we compute SHAP interaction values for all features, we get one matrix per instance with dimensions M x M, where M is the number of features.

How can we use the interaction index?
For example, to automatically color the SHAP feature dependence plot with the strongest interaction:
-->
この方程式は特徴量の主効果を差し引くので、個々の効果を考慮した後の純粋な相互作用効果を得ることができます。
シャープレイ値の計算と同様に、私たちは、考えられる全ての特徴量連合 S の値を平均化します。
全ての特徴量のSHAP相互作用値を計算した時、インスタンスごとに、サイズが M x M の行列が得られます。ここで M は特徴量の数を表しています。

どのように相互作用インデックスを使用するのでしょうか？
例えば、相互作用の強さで自動的に SHAP feature dependence plot をつけます。

<!--
fig.cap = "SHAP feature dependence plot with interaction visualization. Years on hormonal contraceptives interacts with STDs. In cases close to 0 years, the occurence of an STD increases the predicted cancer risk. For more years on contraceptives, the occurence of an STD reduces the predicted risk. Again, this is not a causal model. Effects might be due to confounding (e.g. STDs and lower cancer risk could be correlated with more doctor visits)."
-->

```{r fig.cap = "SHAP feature dependence plot による相互作用の可視化。ホルモン避妊薬の使用年数は性感染症と相互作用する。0年に近づく、STDの発生がある場合は予測される癌のリスクが増加する。避妊薬を数年に渡り使用した場合、STD の発生は予測されたリスクを減少させる。繰り返しになりなるが、これは、因果のモデルではない。この影響は交絡による可能性がある。（例えば、STDsと癌リスクの低下は、より多くの医師の診察と相関している可能性がある。", out.width=800}
knitr::include_graphics("images/shap-dependence-interaction.png")
```

### Clustering SHAP values
<!--
### Clustering SHAP values
-->

<!--
You can cluster your data with the help of Shapley values.
The goal of clustering is to find groups of similar instances.
Normally, clustering is based on features.
Features are often on different scales.
For example, height might be measured in meters, color intensity from 0 to 100 and some sensor output between -1 and 1.
The difficulty is to compute distances between instances with such different, non-comparable features.

SHAP clustering works by clustering on Shapley values of each instance.
This means that you cluster instances by explanation similarity.
All SHAP values have the same unit -- the unit of the prediction space.
You can use any clustering method.
The following example uses hierarchical agglomerative clustering to order the instances.

The plot consists of many force plots, each of which explains the prediction of an instance.
We rotate the force plots vertically and place them side by side according to their clustering similarity.
-->
シャープレイ値を使って、データをクラスタリングできます。
クラスタリングの目的は似たようなインスタンスのグループをみつけることです。
クラスタリングは普通、特徴量を基に行われますが、特徴量が異なるスケールを持つことがよくあります。
例えば、身長はメートルで計測されていたり、色の強弱は 0 から 100 の数字であったり、センサー出力が -1 から 1 の間ということもあります。
そのような異なる比較できない特徴量を持つインスタンスの間の距離を計算することは困難です。

SHAP clustering は各インスタンスのシャープレイ値を使ってクラスタ化します。
これは、説明の類似性によってインスタンスをクラスタ化することを意味します。
全てのSHAP値は同じ単位 -- (予測空間の範囲) -- を持っています。
任意のクラスタ手法が使用可能です。
次の例では、階層的クラスタリングを使ってインスタンスを順序付けています。

プロットは多くの force plot で構成されており、それぞれがインスタンスの予測を説明します。
force plotを垂直に回転させ、クラスタリングの類似性にしたがって並べて配置しています。

<!--
fig.cap="Stacked SHAP explanations clustered by explanation similarity. Each position on the x-axis is an instance of the data. Red SHAP values increase the prediction, blue values decrease it. A cluster stands out: On the right is a group with a high predicted cancer risk."
-->

```{r, fig.cap="説明の類似性によってクラスタリングされた Stacked SHAP explanations。それぞれのx軸の位置はデータのインスタンス。赤の SHAP 値は予測を増加させ、青の値は予測を減少させる。右にある目立つクラスタは、癌のリスクが高いと予測されたグループです。", out.width=800}
knitr::include_graphics("images/shap-clustering.png")
```

### 長所
<!--
### Advantages
-->

<!--
Since SHAP computes Shapley values, all the advantages of Shapley values apply:
SHAP has a **solid theoretical foundation** in game theory.
The prediction is **fairly distributed** among the feature values.
We get **contrastive explanations** that compare the prediction with the average prediction.

SHAP **connects LIME and Shapley values**.
This is very useful to better understand both methods.
It also helps to unify the field of interpretable machine learning.

SHAP has a **fast implementation for tree-based models**.
I believe this was key to the popularity of SHAP, because the biggest barrier for adoption of Shapley values is the slow computation.

The fast computation makes it possible to compute the many Shapley values needed for the **global model interpretations**.
The global interpretation methods include feature importance, feature dependence, interactions, clustering and summary plots.
With SHAP, global interpretations are consistent with the local explanations, since the Shapley values are the "atomic unit" of the global interpretations.
If you use LIME for local explanations and partial dependence plots plus permutation feature importance for global explanations, you lack a common foundation.
-->
SHAPがシャープレイ値を計算する事から、シャープレイ値の全ての長所がSHAPにも反映されます。
SHAPは、ゲーム理論において "確かな理論的根拠" を持ちます。
予測は特徴量の中で**公平に分配**されています。
平均の予測と、個々の予測を比較することで **対照的な説明** ができます。

SHAPは、**LIME と シャープレイ値をつなげます**。
これは、両方の手法をより理解するためにとても有用です。
また、解釈可能な機械学習の分野を統一するために有用です。

SHAPは、**決定木ベースのモデルに対して高速な実装**を持っています。
私はこれが SHAP の人気の鍵だと信じています。
なぜなら、シャープレイ値の実装の一番の障害はその計算速度の遅さだからです。

早い計算速度は、**大域的なモデルの解釈**に必要な多くのシャープレイ値の計算をすることを可能にします。
大域的な解釈の手法は、特徴量重要度(feature importance)、feature dependence、interactions, clustering や summary plots があります。
SHAPを使用すると、シャープレイ値が大域的な解釈の "原子単位(atomic unit)" であるため、大域的な解釈と局所的な解釈が一致します。
もしあなたが LIME を局所的な説明に使い、partial dependence plot、permutation feature importance を大域的な説明のために使用した場合、共通の基盤が欠けています。

### 短所
<!--
### Disadvantages
-->

<!--
**KernelSHAP is slow**.
This makes KernelSHAP impractical to use when you want to compute Shapley values for many instances.
Also all global SHAP methods such as SHAP feature importance require computing Shapley values for a lot of instances.

**KernelSHAP ignores feature dependence**.
Most other permutation based interpretation methods have this problem.
By replacing feature values with values from random instances, it is usually easier to randomly sample from the marginal distribution.
However, if features are dependent, e.g. correlated, this leads to putting too much weight on unlikely data points.
TreeSHAP solves this problem by explicitly modeling the conditional expected prediction.

**TreeSHAP can produce unintuitive feature attributions**.
While TreeSHAP solves the problem of extrapolating to unlikely data points, it introduces a new problem.
TreeSHAP changes the value function by relying on the conditional expected prediction.
With the change in the value function, features that have no influence on the prediction can get a TreeSHAP value different from zero.

The disadvantages of Shapley values also apply to SHAP:
Shapley values can be misinterpreted and access to data is needed to compute them for new data (except for TreeSHAP).
-->
**KernelSHAPは計算に時間がかかります**。
多くのインスタンスに対してシャープレイ値を計算したい時に、KernelSHAP は実用的ではありません。
また、SHAP feature importance などの、全ての大域的な
 SHAP の手法は多くのインスタンスのシャープレイ値を計算することを必要とします。

**KernelSHAPは特徴量の依存関係を無視します**。
ほとんどの他の permutation ベースの解釈手法も同じ問題を抱えています。
特徴量を、ランダムなインスタンスからの値に入れ替えることによって、簡単に周辺分布からのランダムサンプリングを実現できます。
しかしながら、もし、特徴量に依存がある場合(例えば、相関関係がある)、これは、ありそうもないデータ点に過剰な重みを与える事につながります。
TreeSHAP は、条件付きの予測を明示的にモデリングする事で、この問題を解決しています。

**TreeSHAP は直感的ではない特徴量の属性を作り出す可能性があります**。
TreeSHAP が現実的ではないデータ点を作り出してしまう問題を解決できる一方で、別の問題を作り出してしまいます。
TreeSHAP では、value function は条件付き期待予測に従うように変更されています。
そのため、予測に影響を与えない特徴量に対しても、TreeSHAP は非ゼロの値を持つ可能性があります。

シャープレイ値の欠点もまた、SHAPに適用されます。
シャープレイ値は誤解する恐れがあり、新しいデータに対してシャープレイ値を計算するためには、データにアクセスできる必要があります(TreeSHAPを除いて)。

### ソフトウェア
<!--
### Software
-->

<!--
The authors implemented SHAP in the [shap](https://github.com/slundberg/shap) Python package.
This implementation works for tree-based models in the [scikit-learn](https://scikit-learn.org/stable/) machine learning library for Python.
The shap package was also used for the examples in this chapter.
SHAP is integrated into the tree boosting frameworks [xgboost](https://github.com/dmlc/xgboost/tree/master/python-package) and [LightGBM](https://github.com/microsoft/LightGBM).
In R, there is the [shapper](https://modeloriented.github.io/shapper/) and [fastshap](https://github.com/bgreenwell/fastshap) packages.
SHAP is also included in the R [xgboost](https://rdrr.io/cran/xgboost/man/xgb.plot.shap.html) package.
-->
SHAP の論文の著者らは SHAP の Python パッケージの[shap](https://github.com/slundberg/shap) を実装しました。
この実装は Python のための機械学習ライブラリである [scikit-learn](https://scikit-learn.org/stable/) の決定木ベースのモデルに対して機能します。
このパッケージはこの章の例でも使用されました。
SHAP は tree boosting のフレームワークである [xgboost](https://github.com/dmlc/xgboost/tree/master/python-package) と [LightGBM](https://github.com/microsoft/LightGBM)に統合されています。
R では、[shapper](https://modeloriented.github.io/shapper/) と [fastshap](https://github.com/bgreenwell/fastshap) のパッケージがあります。
SHAP は R の[xgboost](https://rdrr.io/cran/xgboost/man/xgb.plot.shap.html) のパッケージにも含まれています。

[^lundberg2016]: Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems. 2017.

[^tree-shap]: Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. "Consistent individualized feature attribution for tree ensembles." arXiv preprint arXiv:1802.03888 (2018).

[^dummy1]: Sundararajan, Mukund, and Amir Najmi. "The many Shapley values for model explanation." arXiv preprint arXiv:1908.08474 (2019).

[^dummy2]: Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. "Feature relevance quantification in explainable AI: A causality problem." arXiv preprint arXiv:1910.13413 (2019). 
